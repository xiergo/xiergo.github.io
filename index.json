[{"content":"\r蛋白结构预测有很多公认的质量评估指标，如：\n$C_{\\alpha}$-RMSD GDT-TS MaxSub TM-Score S-score IS-score 但是复合物模型领域没有。目前主要依靠三个指标：\n$F_{nat}$：Fraction of native interfacial contacts preserved in the interface of the predicted complex.\n界面（interface）：两个分子（受体和配体）的所有距离小于等于5埃的重原子对。\nLRMS: Ligand Root Mean Square deviation calculated for the backbone of the shorter chain (ligand) of the model after superposition of the longer chain (receptor).\niRMS: the receptor-ligand interface in the native strcture is redefined at cutoff (10 A). The backbone atoms of these \u0026ldquo;interface\u0026rdquo; residues is superposed on their equivalents in the predicted complex to compute the iRMS\n根据上面三个指标，CAPRI 评估定义\nCAPRI evaluation\r在这项研究中，作者推导出这样一个连续的质量度量，DockQ，用于对接模型，而不是分类为不同的质量组，将Fnat，LRMS和iRMS组合在一起，产生[0，1]范围内的分数，分别对应于低质量和高质量。DockQ基本上可用于概括原始CAPRI分类\n$$ DockQ(F_{nat}, LRMS, iRMS, d_1, d_2)=\\frac{F_{nat}+RMS_{scaled}(LRMS, d_1)+ RMS_{scaled}(iRMS, d_2)}{3} $$ 其中 $$ RMS_{scaled}(RMS, d)=\\frac{1}{1+(\\frac{RMS}{d})^2} $$ $d_1=8.5A, d_2=1.5A$是scaling factor\n两个数据集上四种质量的模型的DockQ分布\rDockQ和IS-score的散点图\rPrecision vs recall\r","permalink":"https://xiergo.github.io/posts/tech/dockq/","summary":"蛋白结构预测有很多公认的质量评估指标，如： $C_{\\alpha}$-RMSD GDT-TS MaxSub TM-Score S-score IS-score 但是复合物模型领域没有。目前主要依靠三个指标： $F_{nat}$：Fraction of native interfacial contacts preserved in the interface of the predicted complex. 界面（interface）：两个分子（受体和配体）的所有距离小于等于5埃的重原子对。 LRMS: Ligand Root Mean Square deviation calculated for the backbone of the shorter chain (ligand) of the model","title":"DockQ"},{"content":"\r凸函数 对于函数$f(x)$在定义域$D$上连续且可导，对于$\\forall x_1,x_2 \\in D,\\forall t\\in (0,1)$若满足 $$ tf(x_1)+(1-t)f(x_2) \\ge f(tx_1+(1-t)x_2) $$ 当且仅当$x_1=x_2$等号成立，则$f(x)$为凸函数\nJensen不等式离散形式 若$f(x)$是凸函数，对于$\\forall x_1,x_2,\\dots,x_n\\in D,\\forall t_1,t_2,\\dots,t_n\\in (0,1),\\sum_{i=1}^n t_i=1$，有如下不等式： $$ \\sum_{i=1}^n t_i f(x_i) \\ge f(\\sum_{i=1}^n t_i x_i ) $$ 当且仅当$x_1=x_2=\\dots=x_n$时等号成立。\n证明：（使用数学归纳法）\n当$n=2$时，由凸函数定义，不等式显然成立；\n当$n\\le k-1,(k\u0026gt;=3)$时，假设不等式成立，则\n当$n=k$时，有 $$ \\sum_{i=1}^k t_if(x_i)\\ge (1-t_k)f(\\sum_{i=1}^{k-1}\\frac{t_i}{1-t_k} x_i) +t_kf(x_k)\\ge f(\\sum_{i=1}^k t_i x_i) $$\nJensen不等式连续形式 $$ \\int_{D} f(x)p(x)dx=\\mathbb E_{x\\sim p(x)} (f(x)) \\ge f(\\mathbb E_{x\\sim p(x)}(x))=f(\\int_D xp(x)dx) $$\n证明：\n给定$D$上的分布$p(x)$满足$\\int_D p(x)=1$，记$x_0=\\mathbb E_{x\\sim p(x)}(x)=\\int_D xp(x)$,过点$(x_0, f(x_0))$做$f(x)$的切线$l=ax+b$，则根据凸函数的性质，有 $$ f(x)\\ge ax+b $$ 所以 $$ \\mathbb E(f(x)) \\ge \\mathbb E (ax+b)=a\\mathbb E (x)+b=ax_0+b=f(x_0)=f(\\mathbb E(x)) $$\n","permalink":"https://xiergo.github.io/posts/tech/jensen_inequality/","summary":"凸函数 对于函数$f(x)$在定义域$D$上连续且可导，对于$\\forall x_1,x_2 \\in D,\\forall t\\in (0,1)$若满足 $$ tf(x_1)+(1-t)f(x_2) \\ge f(tx_1+(1-t)x_2) $$ 当且仅当$x_1=x_2$等号成立，则$f(x)$为凸函数 Jensen不等式离散形式 若$f(x)$是凸函数，对于$\\forall x_1,x_2,\\dots,x_n\\in D,\\forall t_1,t_2,\\dots,t_n\\in (0,1),\\sum_{i=1}^n t_i=1$，有如下不等式： $$ \\sum_{i=1}^n t_i","title":"Jensen Inequality"},{"content":"","permalink":"https://xiergo.github.io/posts/life/2023%E5%B9%B4%E6%98%A5%E8%8A%82/","summary":"","title":"2023年春节"},{"content":"\r使用AF-multimer对蛋白复合物结构进行预测 绝大多数的蛋白质单链结构预测已经达到很高精度，但是多链蛋白复合物结构预测仍存在挑战。这里，我们发现，使用已知化学计量学（stoichiometry）的多聚体作为输入训练的AlphaFold模型（起名AlphaFold-Multimer，简写AFM）与更改输入的单链AF模型相比，在保持单链链内结构的准确性的同时，链间界面结构的预测准确性显著提高。我们也在包含4446个蛋白复合物的大型数据集上进行预测，并对所有具有低模版一致性的无冗余界面进行评分。对于异源多聚体界面预测，AFM的DockQ\u0026gt;=0.23（成功预测）占比70%，高准确预测（DockQ\u0026gt;=0.8）占比26%，较加linker的AF模型预测同比增长27和14个百分点。对于同源多聚体，成功预测占比72%，高精度占比36%，增长8和7个百分点。\nIntroduction 蛋白复合物对很多生物过程至关重要。AF2在蛋白单体的预测可以达到实验精度。尽管AF2是在蛋白单链上训练的，其中很多单链是从蛋白复合物上拆分下来的，但是它在预测结合co-factor的蛋白上展现出了不错的能力。通过使用伪多聚体（pseudo-multimer，比如通过gap插入或者linker连接各个链）输入到AF2中，可以成功预测多聚体的相互作用。虽然这些研究显示了原本的AF2模型有很强的泛化能力，但是如果训练也在多聚体上性能会不会提高，提高多少仍然是一个值得探讨的问题。\n回顾AlphaFold2。AF2利用的信息有三个：氨基酸序列、MSA、template。核心网络称为Evoformer，包括MSA representation（MR）和pair representation（PR）。其中，PR包含残基相对位置的信息，用来预测残基之间的相对距离（binned distance distribution， distogram）。最后用MR的第一行和PR预测最终的结构。\n本研究从训练和推理将AF2扩展到多链结构预测，使其支持多链特征提取和对称性处理。我们将其命名为AlphaFold-Multimer并展示其相较于其他已有方法更加出色的性能。\nMethods 总的来说，将AF2扩展到AFM做了以下修改：\nloss项考虑相同链的排列对称性（permutation symmetry） 配对单链MSA提取链间遗传信息 引入一种新方法来选择训练残基的subsets 其他的一些小的调整 多链排列比对 （multi-chain permutation alignment） 在同源二聚体中，排列对称性必须考虑到。一个蛋白中一段序列出现多次时，预测结构和真实结构的mapping是任意的，因此我们取最优的同源链的permutation使得预测与真实结构最相近。本文使用了一个简单的启发式算法来找最优的permutation\nMSA构建 对于同源复合物（homomeric complexes），直接对单链MSA复制n遍（n为链的条数），然后将n个相同的MSA从左到右拼在一起。对于异源复合物（heteromeric complexes）：1）将所有链的MSA中可以pair上的序列从左到右连起来；2）对于partial alignments，即只能在一些MSA中pair上而在另一些MSA中不存在pair上的序列，则将没匹配上的用gap代替；3）剩下没有匹配上的序列都用分块对角矩阵的形式，非对角块用gap填充。在训练时，使每个链的MSA中未匹配上的序列被抽到的概率是一样的\nDuring training we bias sampling of the MSA cluster centres such that each chain’s unpaired MSA has an equal probability of being sampled from, regardless of the number of sequences it contains. Sampling of paired versus unpaired sequences is unbiased, such that the probability of selecting a paired versus unpaired row is proportional to their relative occurrence. At test time we achieve slightly better performance by performing unbiased MSA sampling, this may be because it yields the greatest diversity across recycling iterations.\n链间遗传学 Cross-chain genetics ","permalink":"https://xiergo.github.io/posts/tech/alphafold-multimer/","summary":"使用AF-multimer对蛋白复合物结构进行预测 绝大多数的蛋白质单链结构预测已经达到很高精度，但是多链蛋白复合物结构预测仍存在挑战。这里，我们发现，使用已知化学计量学（stoichiometry）的多聚体作为输入训练的AlphaFold模型（起名AlphaFold-Multim","title":"AlphaFold-Multimer"},{"content":"\r$$ \\pmb a^{(1)}=f(\\pmb x^{(1)})=f(\\pmb W^{(1)}\\pmb x^{(0)}) $$\n$$ \\pmb a^{(k)}=f(\\pmb x^{(k)})=f(\\pmb W^{(k)} \\pmb a^{(k-1)})=\\text {sigmoid}(\\pmb{W}^{(k)}\\pmb{a}^{(k-1)}) \\\\k=2,3,\u0026hellip;,K $$\n$$ \\pmb y=\\pmb a^{(K)}=f(\\pmb x^{(K)}) $$\n第k层的i节点的输出值 $$ a_i^{(k)}=\\text{sigmoid}(\\sum_j w_{ij}^{(k)} a_j^{(k-1)}) $$ 对于输出节点$i$，误差项$\\delta_i$如下： $$ \\begin{aligned} \\delta_i\u0026amp;=-\\frac{\\partial \\ell}{\\partial x_i}\\\\ \u0026amp;=-\\frac{\\partial}{\\partial x_i}(\\frac{1}{2}(t_i-y_i)^2)\\\\ \u0026amp;=\\frac{\\partial y_i}{\\partial x_i}(t_i-y_i)\\\\ \u0026amp;=\\frac{\\partial}{\\partial x_i} (\\frac{1}{1+e^{-x_i}})(t_i-y_i)\\\\ \u0026amp;=\\frac{e^{-x_i}}{(1+e^{-x_i})^2}(t_i-y_i)\\\\ \u0026amp;=y_i(1-y_i)(t_i-y_i) \\end{aligned} $$ 对于最后一隐藏层的节点$i$，误差项$\\delta_i$如下： $$ \\begin{aligned} \\delta_i\u0026amp;=-\\frac{\\partial \\ell}{\\partial x_i}\\\\ \u0026amp;=-\\sum_{j\\in \\text{output}}\\frac{\\partial \\ell}{\\partial x_j}\\frac{\\partial x_j}{\\partial a_i}\\frac{\\partial a_i}{\\partial x_i}\\\\ \u0026amp;=a_i(1-a_i)\\sum_{j\\in \\text{output}}w_{ji}\\delta_j \\end{aligned} $$ 更新权重： $$ \\begin{aligned} w_{ij}\u0026rsquo;\u0026amp;=w_{ij}-\\eta \\frac{\\partial \\ell}{\\partial w_{ij}}\\\\ \u0026amp;=w_{ij}-\\eta\\frac{\\partial \\ell}{\\partial x_i}\\frac{\\partial x_i}{\\partial w_{ij}}\\\\ \u0026amp;=w_{ij}+\\eta \\delta_i a_j \\end{aligned} $$\n下面演示一遍完整的计算过程：\n神经网络\r计算$x_4,x_5,x_6,x_7$： $$ x_4=\\sum_{j\\in {1,2,3}} w_{4j}x_j $$ 计算$a_4,a_5,a_6,a_7$： $$ a_4=\\text{sigmoid}(x_4)=\\frac{1}{1+e^{-x_4}} $$ 计算$x_8,x_9$ $$ x_8=\\sum_{j\\in {4,5,6,7}} w_{8j}a_j $$ 计算$y_8,y_9$ $$ y_8=\\text{sigmoid}(x_8) $$ 计算$\\delta_8,\\delta_9$ $$ \\delta_8=y_8(1-y_8)(t_8-y_8) $$ 计算$\\delta_4,\\delta_5,\\delta_6,\\delta_7$ $$ \\delta_4=a_4(1-a_4)\\sum_{j\\in{8,9}} w_{j4}\\delta_j $$ 更新所有$w_{ij}$ $$ w_{ij}\u0026rsquo;=w_{ij}+\\eta\\delta_ia_j $$\n$$ w_{81}\u0026rsquo;=w_{81}+\\eta\\delta_8a_1 $$\n$$ w_{41}\u0026rsquo;=w_{41}+\\eta\\delta_4 x_1 $$\n","permalink":"https://xiergo.github.io/posts/tech/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/","summary":"$$ \\pmb a^{(1)}=f(\\pmb x^{(1)})=f(\\pmb W^{(1)}\\pmb x^{(0)}) $$ $$ \\pmb a^{(k)}=f(\\pmb x^{(k)})=f(\\pmb W^{(k)} \\pmb a^{(k-1)})=\\text {sigmoid}(\\pmb{W}^{(k)}\\pmb{a}^{(k-1)}) \\\\k=2,3,\u0026hellip;,K $$ $$ \\pmb y=\\pmb a^{(K)}=f(\\pmb x^{(K)}) $$ 第k层的i节点的输出值 $$ a_i^{(k)}=\\text{sigmoid}(\\sum_j w_{ij}^{(k)} a_j^{(k-1)}) $$ 对于输出节点$i$，误差项$\\delta_i$如下： $$ \\begin{aligned} \\delta_i\u0026amp;=-\\frac{\\partial \\ell}{\\partial x_i}\\\\ \u0026amp;=-\\frac{\\partial}{\\partial x_i}(\\frac{1}{2}(t_i-y_i)^2)\\\\ \u0026amp;=\\frac{\\partial y_i}{\\partial x_i}(t_i-y_i)\\\\ \u0026amp;=\\frac{\\partial}{\\partial x_i} (\\frac{1}{1+e^{-x_i}})(t_i-y_i)\\\\ \u0026amp;=\\frac{e^{-x_i}}{(1+e^{-x_i})^2}(t_i-y_i)\\\\ \u0026amp;=y_i(1-y_i)(t_i-y_i) \\end{aligned} $$ 对于最后一隐藏层的节点$i$，误差项$\\delta_i$如下： $$ \\begin{aligned} \\delta_i\u0026amp;=-\\frac{\\partial \\ell}{\\partial x_i}\\\\ \u0026amp;=-\\sum_{j\\in \\text{output}}\\frac{\\partial \\ell}{\\partial x_j}\\frac{\\partial x_j}{\\partial a_i}\\frac{\\partial a_i}{\\partial x_i}\\\\ \u0026amp;=a_i(1-a_i)\\sum_{j\\in \\text{output}}w_{ji}\\delta_j \\end{aligned} $$ 更新权重： $$ \\begin{aligned} w_{ij}\u0026rsquo;\u0026amp;=w_{ij}-\\eta \\frac{\\partial \\ell}{\\partial","title":"反向传播"},{"content":"\r举例单层感知机实现逻辑或（OR）：\n$x_1$ $x_2$ $y$ 0 0 0 1 0 1 0 1 1 1 1 1 $$ y=\\pmb w^T\\pmb x+b $$\n$$ \\ell=1/2(y_0-y)^2 $$\n$$ \\pmb w\u0026rsquo;=\\pmb w-\\eta\\frac {\\partial \\ell}{\\partial \\pmb w}\\\\ =\\pmb w-\\eta(y-y_0)\\pmb x\\\\ =\\pmb w+\\eta(y_0-y)\\pmb x $$\n$$ b\u0026rsquo;=b-\\eta(y-y_0)=b+\\eta(y_0-y) $$\n代码实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 from functools import reduce class Perceptron(object): def __init__(self, dim, activator, eta) -\u0026gt; None: self.w = [0.0 for _ in range(dim)] self.b = 0.0 self.activator = activator self.eta = eta def predict(self, x): wx = reduce(lambda a, b: a + b, [wi * xi for wi, xi in zip(self.w, x)]) return self.activator(wx + self.b) def train(self, epoch, features, labels): for _ in range(epoch): for x, y in zip(features, labels): self._train_one_sample(x, y) def _train_one_sample(self, x, y): y_pred = self.predict(x) delta = y - y_pred self.w = [w + self.eta * delta * x for x, w in zip(x, self.w)] self.b += self.eta * delta def __str__(self) -\u0026gt; str: return f\u0026#39;Weight: {self.w}\\nBias: {self.b}\\n\u0026#39; def activator(x): return 1 if x \u0026gt; 0 else 0 def main(): features = [[0, 0], [0, 1], [1, 0], [1, 1]] labels = [0, 1, 1, 1] p = Perceptron(2, activator, 0.01) print(features) print(labels) print(p) p.train(10, features, labels) print(p) y_pred = [p.predict(x) for x in features] print(y_pred) if __name__ == \u0026#34;__main__\u0026#34;: main() 结果\n1 2 3 4 5 6 7 8 9 [[0, 0], [0, 1], [1, 0], [1, 1]] [0, 1, 1, 1] Weight: [0.0, 0.0] Bias: 0.0 Weight: [0.01, 0.01] Bias: 0.0 [0, 1, 1, 1] ","permalink":"https://xiergo.github.io/posts/tech/perceptron/","summary":"举例单层感知机实现逻辑或（OR）： $x_1$ $x_2$ $y$ 0 0 0 1 0 1 0 1 1 1 1 1 $$ y=\\pmb w^T\\pmb x+b $$ $$ \\ell=1/2(y_0-y)^2 $$ $$ \\pmb w\u0026rsquo;=\\pmb w-\\eta\\frac {\\partial \\ell}{\\partial \\pmb w}\\\\ =\\pmb w-\\eta(y-y_0)\\pmb x\\\\ =\\pmb w+\\eta(y_0-y)\\pmb x $$ $$ b\u0026rsquo;=b-\\eta(y-y_0)=b+\\eta(y_0-y) $$ 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 from functools import reduce class Perceptron(object): def __init__(self, dim, activator, eta) -\u0026gt; None: self.w = [0.0 for _ in range(dim)] self.b = 0.0 self.activator = activator self.eta = eta def predict(self, x): wx = reduce(lambda","title":"单层感知机代码实现"},{"content":"\rRNN即循环神经网络（recurrent neural network）。 $$ S_{t}=f(WS_{t-1}+UX_t)\\\\ O_t=g(VS_t) $$\n","permalink":"https://xiergo.github.io/posts/tech/rnn/","summary":"RNN即循环神经网络（recurrent neural network）。 $$ S_{t}=f(WS_{t-1}+UX_t)\\\\ O_t=g(VS_t) $$","title":"RNN"},{"content":"\r评估指标（evaluation metrics） 准确率（ACC）\n精确度（precision）\n召回率（recall）\nF1 Score\n马修斯相关系数（Mathews correlation coefficient, MCC）\n受试者操作特征曲线下面积（AUROC: area under receiver operating characteristic curve）\nAUPRC: area under precision recall curve\n$$ \\text{ACC}=\\frac{TP+TN}{TP+TN+FP+FN}\\\\ \\text{precision}=\\frac{TP}{TP+FP}\\\\ \\text{recall}=\\text{TPR}=\\text{sensitivity}=\\frac{TP}{TP+FN}\\\\ F_1=\\frac{2\\times \\text{precision}\\times\\text{recall}}{\\text{precision}+\\text{recall}}\\\\ \\text{MCC}=\\frac{TN\\times TP-FN\\times FP}{\\sqrt{(TN+FN)\\times(TP+FP)\\times(TP+FN)\\times(TN+FP)}} $$\nAUROC ROC is short for Receiver Operating Characteristic. That name stems from the fact that the methodology was first developed during World War II to evaluate the performance of radar receivers in the detection of enemy aircraft.\n受试者操作特征曲线（receiver operating characteristic curve， ROC），横轴是假阳性率（false positive rate， FPR），1-TNR，1-specificity。 $$ \\text{TNR}=\\text{specificity}=\\frac{TN}{TN+FP} $$\n$$ \\text{FPR}=\\frac{FP}{TN+FP} $$\n纵轴是真阳性率（True positive rate, TPR） $$ \\text{TPR}=\\text{recall}=\\text{sensitivity}=\\frac{TP}{TP+FN} $$ ROC曲线下面积（Area under ROC curve， AUROC）\nROC曲线\rAUPRC AUPRC即PRC曲线下的面积（area under precision-recall curve）。PRC曲线：\n横轴是：Recall，即TPR\n纵轴是：Precision $$ \\text{AUPRC}=\\int_0^1 p(r)\\text dr $$\n曲线下面积也称为平均精度（Average precision, AP）\nROC和PRC的计算实例\r根据上面的结果，分别绘制ROC和PRC曲线\nROC和PRC的绘制\r与AUROC相比，AUPRC更适合用于正负样本不均衡的情形。\n比如，有20个正样本和2000个负样本，则ROC不能很好的区分模型的效果。我们有两个模型，分别是preferred model（性能好） 和 other model（性能差）。如下图所示：\n两个模型的ROC\rpreferred model在其前 20 个预测中找到 80% 的正确值，other model在其前 60个预测中找到 80% 的正确值，但是ROC中，preferred model还比other model更差。而PRC就可以区分preferred model 和other model。\n两个模型的PRC\r","permalink":"https://xiergo.github.io/posts/tech/%E4%BA%8C%E5%88%86%E7%B1%BB%E7%9A%84%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/","summary":"评估指标（evaluation metrics） 准确率（ACC） 精确度（precision） 召回率（recall） F1 Score 马修斯相关系数（Mathews correlation coefficient, MCC） 受试者操作特征曲线下面积（AUROC: area under receiver operating characteristic curve） AUPRC: area under precision recall curve $$ \\text{ACC}=\\frac{TP+TN}{TP+TN+FP+FN}\\\\ \\text{precision}=\\frac{TP}{TP+FP}\\\\ \\text{recall}=\\text{TPR}=\\text{sensitivity}=\\frac{TP}{TP+FN}\\\\ F_1=\\frac{2\\times \\text{precision}\\times\\text{recall}}{\\text{precision}+\\text{recall}}\\\\ \\text{MCC}=\\frac{TN\\times TP-FN\\times FP}{\\sqrt{(TN+FN)\\times(TP+FP)\\times(TP+FN)\\times(TN+FP)}} $$ AUROC ROC is short for Receiver Operating Characteristic. That name stems from the fact that","title":"二分类的评估指标"},{"content":"\r蛋白蛋白相互作用（protein-protein interaction, PPI）位点预测问题在生物化学领域都很重要，过去的方法都很耗时且准确度不高。我们提出RGN（Residue based Graph attention and convolutional Network）来预测PPI位点。我们将蛋白看成一张图，残基看作节点，节点的特征包括四部分：PSSM（the position specific scoring matrix）、隐马尔可夫模型、氢键评估算法、ProtBert。节点之间是否有边取决于残基的空间距离。然后使用GAT（Graph attention network）和GCN（Graph convolutional network）来提取深层特征。最后将特征经过预测层输出。后续的benchmark中，我们的方法在各个尺度上（准确率、精确率、召回率、F1分数、Matthews相关系数、ROC曲线下面积、精确率-召回率曲线下面积）都是最优的。并且，我们进行了case study表明从蛋白结构层面提取的蛋白信息是有效的，同时指出PPI预测问题的难点。\nMethod Node feature 节点特征包括两部分：1）蛋白的物理化学特征：PSSM、HMM、DSSP；2）预训练模型（ProtBert）的特征\nPSSM：PSSM（Position Specific Scoring Matrix）是用PSI-BlAST（Position specific iterative-Basic local alignment search tool）得到的，比对数据库是UniRef90。表示每一个位置20种氨基酸出现的概率。（20维）\nHMM：是通过HHblits得到的。比对数据集是\n","permalink":"https://xiergo.github.io/posts/tech/rgn/","summary":"蛋白蛋白相互作用（protein-protein interaction, PPI）位点预测问题在生物化学领域都很重要，过去的方法都很耗时且准确度不高。我们提出RGN（Residue based Graph attention and convolutional Network）来预测PPI位点。我们将蛋白看成一张图，残基看作节点，节点的特征包括四部分：PSSM（the position specific scoring","title":"RGN:基于残基的GAT和GCN结合预测蛋白互作位点"},{"content":"\r开发了一种测量NMR蛋白结构准确性的方法。该方法比较RCI和通过数学刚性理论从NMR结构中计算得到的局部刚性（local rigidity）的相关性（二级结构）和RMSD（全局刚性，overall rigidity），并且进行后续的评估。\nRCI: random coil index\npredicts protein flexibility by calculating an inverse weighted average of backbone secondary chemical shifts and predicting values of model-free order parameters as well as per-residue RMSD of NMR and molecular dynamics ensembles from this parameter.\nchemical shift：\nIn nuclear magnetic resonance (NMR) spectroscopy, the chemical shift is the resonant frequency of an atomic nucleus relative to a standard in a magnetic field. Often the position and number of chemical shifts are diagnostic of the structure of a molecule. Chemical shifts are also used to describe signals in other forms of spectroscopy such as photoemission spectroscopy.\nSome atomic nuclei possess a magnetic moment (nuclear spin), which gives rise to different energy levels and resonance frequencies in a magnetic field. The total magnetic field experienced by a nucleus includes local magnetic fields induced by currents of electrons in the molecular orbitals (note that electrons have a magnetic moment themselves). The electron distribution of the same type of nucleus (e.g. 1H, 13C, 15N) usually varies according to the local geometry (binding partners, bond lengths, angles between bonds, and so on), and with it the local magnetic field at each nucleus. This is reflected in the spin energy levels (and resonance frequencies). The variations of nuclear magnetic resonance frequencies of the same kind of nucleus, due to variations in the electron distribution, is called the chemical shift. The size of the chemical shift is given with respect to a reference frequency or reference sample (see also chemical shift referencing), usually a molecule with a barely distorted electron distribution.\nMethods 计算RCI RCI， random coil index，对于每个残基可以计算一个局部蛋白灵活度（local protein flexibility），是主链二级化学位移（backbone secondary chemical shift）的加权平均的倒数。 $$ R_{RCI}=\\left ( \\frac{A|\\Delta\\delta_{C\\alpha}|+B|\\Delta\\delta_{CO}|+C|\\Delta\\delta_{C\\beta}|+D|\\Delta\\delta_{N}|+E|\\Delta\\delta_{NH}|+F|\\Delta\\delta_{H\\alpha}|}{A+B+C+D+E+F}\\right )^{-1} $$ $\\Delta\\delta_{I}$代表二级化学位移，A-F代表权重系数。不存在的原子或者没有化学位移数据的原子系数为0。没有氧原子？\n计算FIRST FIRST: Floppy inclusions and rigid substructure topography\n对于一个蛋白质结构，FIRST生成由节点组成的约束网络图。其中节点表示原子，边表示局部几何施加的约束。单共价键用原子之间的五个边表示，双键用六条边表示，疏水相互作用用两条边，氢键用1～5条边。\n原子可以看作刚体，每个原子有六个自由度（三个位置和三个方向）。当有约束存在时，自由度会减少。一条边会减少至多一个自由度。比如，单共价键可以消除两个键合原子之间至多五个自由度。然后，FIRST使用组合卵石博弈算法（combinatorial pebble game algrithom）将图快速分解为最大刚性簇(maximum rigid clusters)和柔性区域(flexible regions)，这一过程称为刚性簇分解(rigid cluster decomposition)。如果Cα原子属于包含至少15个原子的刚性簇，我们认为残基是刚性的，这样做可以防止脯氨酸和芳香族残基自动标记为刚性。\n相对灵活度（relative flexibility）使用氢键稀释（hydrogen bond dilution）的过程进行量化，该过程类似于蛋白质的热变性。逐步去除图中氢键的边，重复刚性簇分解并关注每个残基的Cα原子不再是刚性簇一部分（即变得灵活）时的氢键能量。稀释图的一个重要好处是每个氢键的绝对能量对分析并不重要。我们对此进行了轻微调整，将能量转换为298.15 K的玻尔兹曼种群比（Boltzmann population ratio），以表示残基是柔性（flexible）的概率。\n简单网络的最终鹅卵石覆盖物的示意图。自由鹅卵石直接放置在顶点上，表示自由度 。覆盖边的鹅卵石直接放置在图形的边上，表示距离约束。鹅卵石覆盖并不是唯一的，因为鹅卵石可以一些简单规则重新排列。图中展示了一个鹅卵石交换的示例（两个箭头）。\r对比RCI和FIRST RCI 和 FIRST 的直接比较并不理想，因为 RCI 和 FIRST 输出值的频率分布不同，于是我们对初始RCI值进行了调整，使得RCI和FIRST可比。调整前后的结果如下图所示：\n$$ R_{RCI}^{\u0026rsquo;}=\\min{\\left(\\frac{\\max ((R_{RCI}-0.024),0)}{0.2-0.024},1\\right)} $$\n初始RCI (a) 和 FIRST (b)的输出值的频率分布直方图；CNW数据集(c)和随机sample的PDB数据集(d)上调整后RCI值和FIRST值的分布图\r验证得分 使用Spearman相关系数和RMSD，RMSD计算如下： $$ \\text{RMSD}=\\sqrt{\\frac{\\sum(R_{RCI}^{\u0026rsquo;}-R_{FIRST})^2}{N}} $$ 其中，N是残基数目。相关性评分和RMSD评分的数值表示为相对于参考数据集的百分位数，该参考数据集由RECOORD重新计算的结构数据库中的CNS和CNW数据集构成，这些参考数据集涵盖了显式溶剂细化前后不同折叠类型的代表蛋白结构。\nCode availability 安装及使用 ANSURR: Github\nhttps://www.github.com/nickjf/ANSURR\n安装：\n1 pip install ansurr 使用文档：\n使用文档\rANSURR需要提供两个文件：\na NMR protein structure in PDB format a shifts file in NEF format (or NMR Star v3 format) 比如：\nBeR31_2k2e.nef\rTo re-reference chemical shifts using PANAV before running ANSURR (recommended):\n1 ansurr -p xxxx.pdb -s xxxx.nef -r To run without re-referencing chemical shifts:\n1 ansurr -p xxxx.pdb -s xxxx.nef 输出 一个总的打分文件：score.out score.out：总共有20个models\r所有模型的分数的散点图： 所有模型散点图\r每个模型有一个详细的打分文件： 第3列往后依次是：flexibility predicted by RCI, flexibility predicted by FIRST, secondary structure according to DSSP, well-defined regions of the ensemble according to CYRANGE, backbone chemcial shift completeness and which atom types have chemical shift data\nModel 1 的详细打分文件\r每个模型有一个图： Model 1 的RCI（蓝色）和FIRST（橙色）预测的蛋白质灵活性图。α螺旋和β片二级结构分别用红点和蓝点表示。\r除此之外还有一些分析的中间过程文件\nPANAV/ - re-referenced chemical shifts RCI/ - flexibility predicted from chemical shifts using RCI extracted_pdbs/ - PDB files for each model extracted from the NMR structure DSSP/ - secondary structure for each model according to the program DSSP FIRST/ - flexibility predicted for each model using FIRST 参考文献 A method for validating the accuracy of NMR protein structures Protein flexibility predictions using graph theory ","permalink":"https://xiergo.github.io/posts/tech/ansurr1/","summary":"开发了一种测量NMR蛋白结构准确性的方法。该方法比较RCI和通过数学刚性理论从NMR结构中计算得到的局部刚性（local rigidity）的相关性（二级结构）和RMSD（全局刚性，overall rigidity），并且进行后续的评估。 RCI: random coil index predicts protein flexibility by calculating an inverse weighted average of backbone secondary chemical shifts and predicting values of model-free","title":"ANSURR:一种验证NMR蛋白结构准确性的方法"},{"content":"\rAbstract CASP比赛中，AF2表现很好，其中表现最差的部分是NMR结构。两种可能：1）NMR结构很差，AF2预测优于NMR；2）晶体结构和溶液结构相差悬殊。我们使用ANSURR（Accuracy of NMR Structure Using RCI and Rigidity）衡量溶液结构的准确性。对比NMR结构和AF2结构，发现大部分AF2结构比NMR结构准确，但也有小部分相反，它们主要是一些动态的结构。AF2可以用来做NMR结构优化，ANSURR验证过的AF2不需要结构优化。\nHighlight 对比了904对AF2结构和NMR结构； AF2结构通常比NMR结构更准； NMR结构在局部动态区域准确性高于AF2； NMR可以用来验证AF2结构，可以根据验证结果再来决定是否有必要进行优化以及如何优化。 Introduction 14届CASP比赛中，AF2大部分的GDT_TS（Global Distance Test Total Score）都大于80，中位数92.4。5/93个结构低于70，其中有三个是复合物中的单链，两个是NMR结构。NMR结构一般很小，比较容易预测，为什么AF2预测不好呢？有两种可能：\n1）NMR结构本身比较差，AF2比NMR结构准确；\n2）AF2对于NMR结构预测不准，因为它是在晶体结构上训练的。NMR结构在体温溶液下得到的，而晶体结构是在低温结晶下得到的，二者可能有所不同。\n这样产生以下问题：\n1）AF2预测溶液结构准确性如何？\n2）如果AF2结构优于NMR，是否有必要做NMR？\n3）溶液结构是否真的和晶体结构（或AF2结构）有差异？\n4）高质量的NMR结构是否可以代表真实的溶液结构？如果是，如何做到的？\n没有一个准确的方法判断NMR结构是否接近溶液构象的平均。验证NMR结构是否准确的方法是将其与晶体结构进行对比。之前的研究证明NMR结构和晶体结构接近。但是如果溶液结构和晶体结构有差异，这种比较就是不对的。\n我们之前开发了一个工具ANSURR，可以将蛋白结构的局部刚性（local rigidity）和基于主链NMR化学位移的random coil index测量得到的局部刚性做对比。\n本文大纲如下：\n1）从全局准确性和局部准确性对比3个NMR结构和相应的CASP14中AF2预测的结构；\n2）比较904个AF蛋白结构数据库中的结构和PDB中的对应的NMR结构，关注各自优于对方的结构案例；\n3）探究AF2预测准确性和ANSURR得到的准确性的关系。\n我们发现大部分AF2预测结构优于NMR结构，少部分涉及动态平均的结构中NMR结构更优。\nResults NMR结构和AF2结构的准确性 ANSURR的原理：\n通过两种策略计算蛋白的灵活度：1）基于主链化学位移；2）从结构出发，基于刚性的数学方法。计算两种方法得到的灵活度的Spearman相关系数和RMSD。PDB中所有NMR结构的上面两个值的百分位数用来计算两个分数：相关性分数和RMSD分数。画出散点图，则位于右上角的结构更好。\n如图1所示，三个NMR结构中，T1055，NMR$\\approx$AF2；T1027，NMR\u0026gt;AF2；T1029，AF2\u0026raquo;NMR。\nFig 1. CASP14的3个NMR结构的ANSURR得分\rT1027 从Fig 1看出，对于T1027，NMR系综比所有的预测结构准确。其中AF2预测结构优于其他预测结构。\nFig 2. T1027的ANSURR分析\rT1027一共有四个模糊区（ill-defined region），ANSURR根据主链化学位移分析的灵活度显示第二个模糊区的灵活度比其他三个模糊区小。其他三个是动态（dynamic）且无规则的（intrinsically disorder），ANSURR的结果暗示，第二个模糊区是动态的，但并不是无规则的。同时第二个区域中间有一小段灵活度降低。这两点在NMR结构中都有所体现，而在AF2中则没有。NMR结构在第二段中间有一小段$\\alpha$-helix。\n化学位移显示第三个模糊区高度无规则，中间有一小段灵活度降低。而NMR结构中，该段完全无规则，中间小的灵活度降低是由于两个弱的氢键。而在AF2的结构中，该部分是turn连接的两段松散的类似$\\beta$-sheet的结构。真正的溶液结构似乎介于灵活的NMR结构和僵硬（rigid）的AF2结构之间。\n第四段，AF2有一部分是helix结构，而NMR结构则没有。ANSURR分析该部分应该是高度灵活的。因为化学位移代表的是一个平均，因此可能该部分在溶液中有helix构象，但不是主要构象。\nT1029 T1029预测的结构GDT_TS最高只有45。然而ANSURR分析结果显示，NMR结构都不如大部分预测的结构，51%的预测结构比最好的NMR结构还要好。Huang et al. 用NOESY谱产生NMR结构与原NOESY谱相比有很多峰（peaks）缺失，暗示该NMR结构确实不准。重新挑选NOESY谱的peaks，重新计算结构，并用AF2结构进行优化（refinement）（称为inverse structure determination），得到的结构和AF2结构比较接近，并且ANSURR分数也有所提高，但是依然低于AF2结构。\n对比人类所有NMR结构和对应的AF2结构 对比了904个PDB中的NMR结构和AFDB中的AF2结构，并用ANSURR对其进行了评估。为了方便，将相关性得分和RMSD得分相加，得到最终的ANSURR得分。\nFig 3. AF2的ANSURR分数-NMR的ANSURR分数的分布\r如图三所示，A图所示是AF2的分数和NMR分数的系综平均比较，可以看出AF2明显优于NMR（差异均值为28）。之前研究显示，NMR系综里的结构差异较大，因此B图显示了AF2和NMR系综最高得分的模型的差异，两者准确性接近（差异均值为2）。图C根据二级结构分别比较了AF2和NMR系综平均，发现$\\beta$-sheet，AF2优于NMR程度最大（差异均值45），其次是$\\alpha$-helix/$\\beta$-sheet混合部分（差异均值29），最小的是$\\alpha$-helix（差异均值17）。这种现象是合理的：由于$\\alpha$-helix在局部几何形状上的变化有限，因此氢键（对于赋予刚性很重要）在细化过程中相对容易获得。 相比之下，$\\beta$-sheet可以采用更广泛的局部几何形状，这使得正确解析氢键更具挑战性。之前研究表明， NMR 结构在 $\\beta$-sheet中通常缺乏氢键。\n对于新的蛋白质，非专家可以在几分钟内生成AF2结构，而NMR结构通常需要数月的专业技能和设备。因此，一个简单的结论是AF2更快，更便宜，至少同样准确，因此应该是生成结构模型的首选方法。然而，现实情况更加微妙，下面更详细地比较两种方法。\nAF2明显优于NMR的案例 AF2-NMR的ANSURR分数超过50的样例有273个，占比30%。这些样例表明，AF2有这明显多的氢键网络，这些氢键让结构更rigid。而NMR结构相比之下氢键数目少，结构更松散。之前研究表明通过增加NMR结构中的氢键可以有效提高ANSURR分数，前提是这些氢键加的位置必须正确。因此AF2可以正确的预测这些氢键的位置。\nFig 4. AF2明显优于NMR的两个代表蛋白的ANSURR输出。结构图中绿色是NMR，青色是AF2，灰色短棒是氢键。折线图顶部的颜色代表二级结构：蓝色beta sheet，红色是alpha helix。蓝色曲线代表从化学位移中计算的灵活度，橙色曲线代表从各自结构中计算的灵活度。\r如图四所示，A，B分别是肌动蛋白B（Filamin）的NMR结构和AF2结构。C，D是锌指蛋白（zinc finger protein）的NMR和AF2结构。AF2结构更rigid，而且更贴合化学位移计算的灵活度。\nNMR明显优于AF2的案例 只有22例NMR-AF2\u0026gt;=50的样本，占比（2%）。有三个原因导致NMR优于AF2：\n1）NMR仅测了整个蛋白的一部分（比如单个结构域），比如在Fig5A/B中，由于NMR只测了一部分，C末端的灵活度上升，如果在一个整体结构中，该部分的灵活度无法达到这么高。而AF2是在整体预测该蛋白，然后截取该部分，因此C末端更加rigid。\n2）AF2无法预测会漏掉一些二级结构，比如Fig5A/B中，AF2有两小段$\\beta$-sheet没有预测出来，导致该部分的灵活度很高与实际情况不符，但是AF2在该区域的pLDDT很低（66）。\n3）AF2会错误预测一些不存在的二级结构。如Fig5C/D，AF2将两段alpha helices连成一个alpha helix。这可能是因为AF2训练集中很少有这种alpha helix中间有一小段break的结构。但是和上面一样，该区域的pLDDT也很低（58）。\nFig 5. NMR明显优于AF2的两个代表蛋白的ANSURR输出。结构图中绿色是NMR，青色是AF2，灰色短棒是氢键。折线图顶部的颜色代表二级结构：蓝色beta sheet，红色是alpha helix。蓝色曲线代表从化学位移中计算的灵活度，橙色曲线代表从各自结构中计算的灵活度。\r比较pLDDT和ANSURR分数 Fig 6. pLDDT和ANSURR比较，B是所有的，C是well defined\rregions\r从图6看出，pLDDT和ANSURR没有明显的相关性。\n对于一些ill-defined 区域（一般是由于局部动态），AF2容易将其识别为有结构，然后给一个低的pLDDT，而NMR倾向于有部分结构（partial structure）。这也和之前的研究相符：AF2很难预测出由点突变引起的结构松散（local unfolding）。\nDiscussion 通过比较AF2结构与溶液结构的化学位移得出，AF2预测溶液结构的准确性很高。然而也有一些预测结构不如NMR结构的情况。 计算AF2结构代价很小，但是解一个NMR结构则耗时耗力。但是小型或中型蛋白的主链NMR指认（assignment）流程已经可以自动化。 对于溶液结构，可以先用AF2预测，在做一个半自动化的NMR骨架指认，然后计算ANSURR分数。如果分数高的可以直接当做准确的结构；对于分数低的，可以在AF2结构的基础上根据ANSURR的结果进行结构优化。比如：在图5A/B中，ANSURR的分析结构显示AF2预测的结构少了一小段反平行的$\\beta$-sheet结构，所以我们可以通过加一些额外的约束在AF2预测结构的基础上增加这么一段$\\beta$-sheet。 Methods 可比的NMR和AF2结构 使用SIFTS（Structure Integration with Function, Taxonomy）来匹配AFDB的Uniport Id和pdb中的NMR结构。使用uniprot_segment_observed.tsv SIFTS文件来鉴别AF2结构和NMR结构的共有区域，并用in-house软件提取共有区域。使用REDUCE v3.23对AF2预测结构加氢。对NMR结构做如下筛选：\n1）只要一条单链的NMR结构\n2）有一组主链的化学位移至少有75%完整度，来确保ANSURR分析可靠\n3）至少有20个氨基酸残基\n最终得到904个AF2/NMR结构对。\nANSURR计算 ANSURR计算使用ANSURR v1.1.0。\n参数如下：\n1）Re-reference chemical shift: PANAV\n2）计算灵活度包括非标准残基（non-standard residue），不包括配体（ligand）。\nNMR结构一般有多个model（～20），对于每一个PDB entry，计算所有model的ANSURR分数，然后取均值。一个AF2结构可能对应多个PDB entries。此情况下，就取多个PDB entries的ANSURR均值，将其与AF2对应区域的ANSURR分数均值进行比较。\n参考文献 The accuracy of protein structures in solution determined by AlphaFold and NMR ","permalink":"https://xiergo.github.io/posts/tech/%E4%BD%BF%E7%94%A8af2%E5%92%8Cnmr%E7%A1%AE%E5%AE%9A%E7%9A%84%E6%BA%B6%E6%B6%B2%E4%B8%AD%E8%9B%8B%E7%99%BD%E7%BB%93%E6%9E%84%E7%9A%84%E5%87%86%E7%A1%AE%E6%80%A7/","summary":"Abstract CASP比赛中，AF2表现很好，其中表现最差的部分是NMR结构。两种可能：1）NMR结构很差，AF2预测优于NMR；2）晶体结构和溶液结构相差悬殊。我们使用ANSURR（Accuracy of NMR Structure Using RCI and Rigidity）衡量溶液结构的准确性。对比NMR结构和AF2结构，发现大部分A","title":"使用AF2和NMR确定的溶液中蛋白结构的准确性"},{"content":"\r问题描述 有一些重复的任务（cpu任务），输入不同，希望在slurm调度系统下进行并行。\n解决方案 cpu_test.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import time import os import multiprocessing as mp # define function def myfun(a, b): time.sleep(10) return a+b # input args = [[1, 2], [3, 4], [5, 6]] # use all available cores t0 = time.time() num_of_cores = int(os.environ[\u0026#39;SLURM_CPUS_PER_TASK\u0026#39;]) # num_of_cores = mp.cpu_count() # if not on slurm, use this instead with mp.Pool(num_of_cores) as pool: results = pool.starmap(myfun, args) print(f\u0026#39;results: {results}\u0026#39;) print(f\u0026#34;Num of cores: {num_of_cores}\u0026#34;) print(f\u0026#34;Time: {time.time() - t0} sec\u0026#34;) # use one core for comparison num_of_cores = 1 t0 = time.time() with mp.Pool(num_of_cores) as pool: results = pool.starmap(myfun, args) print(f\u0026#39;results: {results}\u0026#39;) print(f\u0026#34;Num of cores: {num_of_cores}\u0026#34;) print(f\u0026#34;Time: {time.time() - t0} sec\u0026#34;) slurm文件\nrun.sh\n1 2 3 4 5 6 7 #!/usr/bin/bash #SBATCH -p cpu1 #SBATCH -c 32 #SBATCH -o run.log python cpu_test.py 提交任务\n1 sbatch run.sh 结果 run.log\n1 2 3 4 5 6 results: [3, 7, 11] Num of cores: 32 Time: 11.071450233459473 sec results: [3, 7, 11] Num of cores: 1 Time: 30.03756856918335 sec 可以看到当使用多个cpu时，用的时间会缩短。\n","permalink":"https://xiergo.github.io/posts/tech/slurm_cpu_%E5%B9%B6%E8%A1%8C/","summary":"问题描述 有一些重复的任务（cpu任务），输入不同，希望在slurm调度系统下进行并行。 解决方案 cpu_test.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import time import os import multiprocessing as mp # define function def myfun(a, b): time.sleep(10) return a+b # input args = [[1, 2], [3, 4], [5, 6]] # use all available cores t0 = time.time() num_of_cores = int(os.environ[\u0026#39;SLURM_CPUS_PER_TASK\u0026#39;]) # num_of_cores = mp.cpu_count() # if not on slurm, use this instead with mp.Pool(num_of_cores) as pool: results = pool.starmap(myfun, args) print(f\u0026#39;results: {results}\u0026#39;) print(f\u0026#34;Num of","title":"Slurm Cpu 并行"},{"content":"\r文献 Title\nSingle-cell meta-analyses reveal responses of tumor-reactive CXCL13+ T cells to immune-checkpoint blockade\nAuthor:\nBaolin Liu , Yuanyuan Zhang, Dongfang Wang, Xueda Hu and Zemin Zhang\nAbstract\nImmune-checkpoint blockade (ICB) therapies represent a paradigm shift in the treatment of human cancers; however, it remains incompletely understood how tumor-reactive T cells respond to ICB across tumor types. Here, we demonstrate that measuring CXCL13 expression could effectively identify both precursor and terminally differentiated tumor-reactive CD8+ T cells within tumors. Applying this approach, we performed meta-analyses of published single-cell data for CXCL13+CD8+ T cells in 225 samples from 102 patients treated with ICB across five cancer types. We found that CXCL13+CD8+ T cells were correlated with favorable responses to ICB, and the treatment further increased such cells in responsive tumors. In addition, CXCL13+ tumor-reactive subsets exhibited variable responses to ICB in distinct contexts, likely due to different degrees of exhaustion-related immunosuppression. Our integrated analyses provide insights into mechanisms underlying ICB and suggest that bolstering precursor tumor-reactive CD8+ T cells might provide an effective therapeutic approach to improve cancer treatment.\n国内外现状 免疫检查点阻断（immune checkpoint blockade, ICB）的内在机制尚不完全清楚，阻碍了免疫治疗的发展。\n单细胞技术的发展以及单细胞数据的积累可以用于刻画经过ICB治疗之后的不同肿瘤中T细胞的动态变化和对ICB的反应。\nBCC和乳腺癌的研究显示PD-1阻断治疗有效的患者耗竭的CD8+T(T_ex)细胞增多，而NSCLC中只有T_ex前体细胞对PD-1阻断有反应。\nBCC中治疗后的产生了新的肿瘤特异型CD8+T克隆，但在乳腺癌和肺癌中并不常见。\n由于肿瘤中大量非肿瘤特特异T细胞（bystanders）的存在，导致单细胞观察到的T细胞行为不能很好地反应肿瘤特异T细胞的行为。\n很多研究用耗竭相关基因（如CD39）来区分bystander和肿瘤特异T细胞。但是在ICB治疗患者中，大量肿瘤特异T细胞由于ICB的影响，耗竭相关基因表达下调以致于无法区分bystander和肿瘤特异T细胞。\n传统用于预测抗pd1和抗pd-l1的疗效用肿瘤突变负担（TMB）,但是效果不理想。\n研究问题 ICB的基本机制是什么？在不同肿瘤中，肿瘤特异性T细胞对ICB的反应是什么？ 如何定义特异性CD8+T细胞的分化状态。 克隆替代现象背后的机制是什么？为什么不同肿瘤中出现该现象的频率不一样？ 是否可以找到比TMB更好的指标来预测不同癌症病人是否对ICB治疗有效。 是否可以找到比耗竭相关基因更可靠的Marker来区分bystander和肿瘤特异T细胞？ 方法 抗原共培养鉴别治疗前的bystander和肿瘤特异T细胞。利用抗原特异性转移策略得到对应的治疗后的抗原特异性。结合单细胞转录组学分析，比如差异表达基因分析（DEG）得出治疗前后均可用CXCL13来区分肿瘤特异细胞。 通过整合多个癌种数据集进行Meta分析，得到不同背景下的T细胞对ICB的反应。 通过相关性分析，得到肿瘤特异细胞丰度和ICB疗效的正相关。 对CXCL13+CD8+T细胞进行无监督聚类（Leiden clustering）和PCA降维将其分为四类，通过对比各类的signature得分以及结合RNA velocity analysis和scATAC-seq数据得到肿瘤特异T细胞的发育轨迹。 对TIL得分和前体样细胞比例的相关性分析，得到肿瘤内部免疫抑制程度是ICB疗效不同的重要潜在因素 对外周血和CD4T细胞也进行了类似的生信分析。 此外用到的统计方法有：方差分析，T-检验，Wilcoxon检验 结果 不管是耗竭T细胞还是非耗竭T细胞，都可以用CXCL13的表达来鉴别肿瘤特异的T细胞，bystander几乎不表达CXCL13 在多个癌种中，PD-1和PD-L1阻断疗法的疗效和CXCL13+的肿瘤特异性T细胞的丰度有着很强的正相关性。 各个癌种均可以通过无监督聚类分为四类：增殖类、终末分化类和两个前提样类（IL7R+HAVCR2-和GZMK+HAVCR2-）。其中肿瘤特异T细胞包括前体样细胞和终末分化细胞。分化方向是IL7R+HAVCR2-到GZMK+HAVCR2-到终末分化细胞。 不同的环境下T细胞对ICB的反应也不同，表现为各肿瘤的前体样细胞占总的CXCL13+CD8+T细胞的比例不一样。进一步发现肿瘤内部免疫抑制程度（CIL得分）和前体样细胞占比呈负相关。肿瘤内部免疫抑制程度是ICB疗效不同的重要潜在因素。低免疫抑制ICB增加前体样细胞，高免疫抑制ICB倾向增加终末分化T细胞。但ICB有效的肿瘤样本中总的肿瘤特异T细胞都会增加。 外周血的肿瘤特异CD8+T细胞的丰度也和ICB疗效正相关。肿瘤内的肿瘤特异T细胞可以被肿瘤外的新的或已经存在的克隆补充上，在ICB有效的病人上更明显。 CXCL13+CD4+T细胞也可提高ICB（或与化疗结合）疗效，用CXCL13+CD4+T细胞和CXCL13+CD8+T细胞一起预测ICB疗效的准确性更好。 重要性和意义 找到一个更可靠的同时适用于ICB前后的分辨肿瘤特异T细胞的marker，CXCL13。 有利于合理设计某些T细胞疗法来治疗肿瘤，比如engineered TCR-T cell therapy。 提供了一个详细的细胞图谱来帮助更好地理解肿瘤特异性T细胞对ICB疗法的反应。 定义了一个统一的适用于多种癌症的肿瘤特异性CD8+T细胞的发育框架，可以单独使用或者和之前研究定义的全局T细胞框架联合使用，帮助更好地理解肿瘤免疫。 发现肿瘤特异T细胞是ICB疗效的关键，暗示肿瘤可能通过降低该细胞来抵抗ICB 肿瘤特异前体T细胞对ICB反应最为明显，提示可以通过抑制前体细胞向终末分化系细胞转变来改善肿瘤治疗。 外周血的肿瘤特异CD8+T细胞的丰度也和ICB疗效正相关，可以利用这个开发基于血液的监测ICB疗效的试剂盒。 ","permalink":"https://xiergo.github.io/posts/tech/%E7%BB%BC%E5%90%88%E8%80%83%E8%AF%9520221215%E6%80%BB%E7%BB%93/","summary":"文献 Title Single-cell meta-analyses reveal responses of tumor-reactive CXCL13+ T cells to immune-checkpoint blockade Author: Baolin Liu , Yuanyuan Zhang, Dongfang Wang, Xueda Hu and Zemin Zhang Abstract Immune-checkpoint blockade (ICB) therapies represent a paradigm shift in the treatment of human cancers; however, it remains incompletely understood how tumor-reactive T cells respond to ICB across tumor types. Here, we demonstrate that measuring CXCL13 expression could effectively identify both precursor and terminally differentiated tumor-reactive CD8+ T cells within tumors. Applying this approach, we performed meta-analyses of published single-cell data for CXCL13+CD8+ T cells in 225 samples from 102 patients treated with ICB across five cancer types. We found that CXCL13+CD8+ T cells were correlated with favorable responses to ICB, and the treatment further increased such cells in responsive tumors. In addition, CXCL13+ tumor-reactive subsets exhibited variable responses to ICB in","title":"综合考试20221215总结"},{"content":"\r今天是2022年12月9号，农历冬月十六。今天是biopic年会，是一二·九师生合唱，也是我的第27个生日。\n与以往一样，爷爷奶奶送上祝福，爸爸妈妈送上祝福，两个妹妹送上祝福\u0026hellip;\u0026hellip;\n不同的是，今年的生日比以往都更有仪式感。有蛋糕，有许愿，有生日歌，有生日礼物，有长寿面，有一桌子美味，有人一起拍照\u0026hellip;\u0026hellip;\n今年生日的晚饭是女朋友花了三个小时准备的。我们昨晚就已经从超市买好了大虾、鸡翅和一系列其他的配菜。女朋友做的油焖大虾和可乐鸡翅超级好吃。感谢你精心准备的生日宴，更感谢你的陪伴。\n精彩瞬间\r今年的一二·九师生合唱筹备了一个多月，遗憾的是，由于疫情取消了最后的合唱比赛。但是今天晚上，我收到了来自师生歌会的老师和同学的祝福。他们提前找了我的亲人和朋友录了一段视频，非常用心。看到视频的一刻，我很惊喜，很感动。非常好奇他们是怎么知道我今天生日的，因为我从来没有跟任何人说过我的生日，而且我过的是农历生日，而身份证上的一般都默认是阳历。问了发我视频的同学，他并没有透露细节。尽管如此，我还是非常开心。在这里感谢准备视频的老师和同学，谢谢你们。\n\u003c!DOCTYPE HTML\u003e\r","permalink":"https://xiergo.github.io/posts/life/%E6%88%91%E7%9A%84%E4%BA%8C%E5%8D%81%E4%B8%83%E5%91%A8%E5%B2%81%E7%94%9F%E6%97%A5/","summary":"今天是2022年12月9号，农历冬月十六。今天是biopic年会，是一二·九师生合唱，也是我的第27个生日。 与以往一样，爷爷奶奶送上祝福，爸爸妈妈送上祝福，两个妹妹送上祝福\u0026hellip;\u0026hellip; 不同的是，今年的生日比以往都更有仪式感。有蛋糕，有许愿，有生日歌，有生日礼物","title":"我的二十七周岁生日"},{"content":"\r这一周用了若干个（4~5）晚上和女朋友将“双人成行”这款游戏通关了。用一句话概括就是：过程无比曲折，游戏非常不错。\n这个游戏在其他平台早就有了，前不久刚登上switch平台。游戏讲述了一对即将离婚的夫妻，因为女儿对着一本魔法书许愿他们和好，灵魂意外附身到女儿的两个玩偶身上，然后这对夫妻操纵玩偶经历各种难关，重新和好，回到现实世界。游戏设计巧妙，包含各种游戏元素，各个关卡的玩法都不尽相同。\n游戏关卡图\r这种3d游戏玩的不熟的，经常视角切换不熟练。我游戏玩得比较多，觉得还行。但是女朋友几乎没咋玩过游戏，玩起来相当地吃力。不过后面也逐渐上手。尽管我们非常艰难地通关，但是游戏结束后仍然对过程十分回味。希望以后还有这种好的游戏可以玩。\n一家三口\r","permalink":"https://xiergo.github.io/posts/life/%E5%8F%8C%E4%BA%BA%E6%88%90%E8%A1%8C/","summary":"这一周用了若干个（4~5）晚上和女朋友将“双人成行”这款游戏通关了。用一句话概括就是：过程无比曲折，游戏非常不错。 这个游戏在其他平台早就有了，前不久刚登上switch平台。游戏讲述了一对即将离婚的夫妻，因为女儿对着一本魔法书许愿他们和好，灵魂意外附身到女儿的两个玩偶身上，然后这对","title":"双人成行"},{"content":"\r变分法可以用来求泛函的极值问题，即：\n找一个满足边界条件$y(x_1)=y_1;y(x_2)=y_2$的函数$y=y(x)$使得泛函$I=\\int_{x_1}^{x_2}F(x,y,y\u0026rsquo;)\\text dx$达到极值点。\n这就是欧拉-拉格朗日方程解决的问题。\n证明：Euler-Lagrange Equation 解：\n设$y=f(x)$满足边界条件： $$ y(x_1)=y_1\\\\ y(x_2)=y_2\\\\ $$ 且泛函$I=\\int_{x_1}^{x_2}F(x,y,y\u0026rsquo;)dx$达到极值点\n引入$\\eta(x)$满足$\\eta(x_1)=\\eta(x_2)=0$且有连续二阶导，则$\\bar y(x)=y(x)+\\varepsilon \\eta(x)$代表一类满足边界条件的函数族。\n这里$\\varepsilon\\eta(x)$就是$y(x)$的微小变化量，称做$y(x)$的变分（variational），记作$\\delta y$。\n那么问题变成找一个特定的$\\bar y(x)$使得$I(\\varepsilon)=\\int_{x_1}^{x_2}F(x,\\bar y, \\bar y\u0026rsquo;)$达到极值点。因为$\\varepsilon=0$是$I(\\varepsilon)$的极值点，所以： $$ \\frac{\\text dI}{\\text d\\varepsilon}\\bigg \\vert_{\\varepsilon=0}=0\\\\ $$ $$ \\frac{\\text d}{\\text d \\varepsilon} \\bigg \\vert_{\\varepsilon=0} \\int_{x_1}^{x_2}F(x,\\bar y, \\bar y\u0026rsquo;)\\text dx=0\\\\ $$ $$ \\int_{x_1}^{x_2}\\frac{\\partial F(x,\\bar y, \\bar y\u0026rsquo;)}{\\partial \\varepsilon}\\bigg \\vert_{\\varepsilon=0}\\text dx=0\\\\ $$ $$ \\int_{x_1}^{x_2}\\bigg [\\frac{\\partial F}{\\partial \\bar y}\\eta+\\frac{\\partial F}{\\partial \\bar y\u0026rsquo;}\\eta\u0026rsquo;\\bigg ]\\bigg \\vert_{\\varepsilon=0}\\text dx=0 \\tag{1} $$\n由分部积分可得\n$$ \\begin{aligned} \u0026amp;\\int_{x_1}^{x_2}\\frac{\\partial F}{\\partial \\bar y\u0026rsquo;}\\eta\u0026rsquo;\\text dx\\\\ =\u0026amp;\\int_{x_1}^{x_2}\\frac{\\partial F}{\\partial \\bar y\u0026rsquo;}\\text d\\eta\\\\ =\u0026amp;-\\int_{x_1}^{x_2}\\eta\\frac{\\text d}{\\text d x}\\bigg [\\frac{\\partial F}{\\partial \\bar y\u0026rsquo;}\\bigg ]\\text dx+\\frac{\\partial F}{\\partial \\bar y\u0026rsquo;}\\eta \\bigg\\vert_{x_1}^{x_2}\\\\ =\u0026amp;-\\int_{x_1}^{x_2}\\eta\\frac{\\text d}{\\text d x}\\bigg [\\frac{\\partial F}{\\partial \\bar y\u0026rsquo;}\\bigg ]\\text dx \\end{aligned} $$\n代入(1)式，则 $$ \\int_{x_1}^{x_2}\\bigg [\\frac{\\partial F}{\\partial \\bar y}-\\frac{\\text d}{\\text dx}\\bigg(\\frac{\\partial F}{\\partial \\bar y\u0026rsquo;}\\bigg)\\bigg ]\\eta\\bigg \\vert_{\\varepsilon=0}\\text dx=0 $$ 在$\\varepsilon=0$时，$\\bar y = y,\\bar y\u0026rsquo;= y\u0026rsquo;$，则： $$ \\int_{x_1}^{x_2}\\bigg [\\frac{\\partial F}{\\partial y}-\\frac{\\text d}{\\text dx}\\bigg(\\frac{\\partial F}{\\partial y\u0026rsquo;}\\bigg)\\bigg ]\\eta\\text dx=0 $$ 因为上式对$\\forall \\eta(x)$都成立，因此 $$ \\bigg[\\frac{\\partial F}{\\partial y}-\\frac{\\text d}{\\text dx}\\bigg(\\frac{\\partial F}{\\partial y\u0026rsquo;}\\bigg)\\bigg ]=0 \\tag{\\text{Euler-Lagrange Equation}} $$\n","permalink":"https://xiergo.github.io/posts/tech/%E5%8F%98%E5%88%86%E6%B3%95/","summary":"变分法可以用来求泛函的极值问题，即： 找一个满足边界条件$y(x_1)=y_1;y(x_2)=y_2$的函数$y=y(x)$使得泛函$I=\\int_{x_1}^{x_2}F(x,y,y\u0026rsquo;)\\text dx$达到极值点。 这就是欧拉-拉格朗日方程解决的问题。 证明：Euler-","title":"变分法"},{"content":"\r扩散过程/前向过程（从右到左）逆扩散过程/重建过程（从左到右）\r前向过程 前向过程是一个加噪音的过程： $$ q(x_t|x_{t-1}):=\\mathcal N(x_t;\\sqrt{\\alpha_t}x_{t-1},\\beta_t\\bold I) $$ 即： $$ x_t=\\sqrt{\\alpha_t}x_{t-1}+\\sqrt{\\beta_t}\\epsilon_t $$ 其中，$\\epsilon_t\\sim\\mathcal N(0,I)$是噪音，$\\sqrt{\\beta_t}\\in(0,1)$是噪声因子，$\\sqrt{\\alpha_t}\\in(0,1)$是衰减系数。逐项展开得到： $$ \\begin{aligned} x_t\u0026amp;=\\sqrt{\\alpha_t} x_{t-1}+\\sqrt{\\beta_t}\\epsilon_t\\\\ \u0026amp;=\\sqrt{\\alpha_t} (\\sqrt{\\alpha_{t-1}} x_{t-2}+\\sqrt{\\beta_{t-1}}\\epsilon_{t-1}) +\\sqrt{\\beta_t}\\epsilon_t\\\\ \u0026amp;=x_0\\prod_{i=1}^t \\sqrt{\\alpha_i} +\\sum_{i=1}^t \\bigg(\\sqrt{\\beta_i}\\epsilon_i\\prod_{j=i+1}^t \\sqrt{\\alpha_j}\\bigg)\\\\ \u0026amp;=\\sqrt{\\bar\\alpha_t}x_0+\\sqrt{\\bar\\beta_t}\\bar \\epsilon \\end{aligned} $$ 这是因为$\\epsilon_i,i\\in{1,\u0026hellip;,t}$是独立同分布的标准正态分布，$\\sum_{i=1}^t \\big(\\sqrt{\\beta_i}\\epsilon_i\\prod_{j=i+1}^t \\sqrt{\\alpha_j}\\big)$ 相当于多个正态分布相加因此可以写成一个正态分布$\\sqrt{\\bar \\beta_t}\\bar\\epsilon$。其中： $$ \\begin{aligned} \\sqrt{\\bar\\alpha_t}:\u0026amp;=\\prod_{i=1}^t \\sqrt{\\alpha_i}\\\\ \\sqrt{\\bar\\beta_t}:\u0026amp;=\\sqrt{\\sum_{i=1}^t \\bigg(\\beta_i\\prod_{j=i+1}^t \\alpha_j\\bigg)}\\\\ \u0026amp;=\\sqrt{\\beta_t+\\alpha_t(\\beta_{t-1}+\\alpha_{t-1}(\u0026hellip;(\\beta_2+\\alpha_2(\\beta_1))))} \\end{aligned}\\\\ \\bar\\varepsilon\\sim\\mathcal N(0,I) $$ 如果把系数的平方加和可得： $$ \\bar \\alpha_t+\\bar \\beta_t=\\prod_{i=1}^t \\alpha_i+\\beta_t+\\alpha_t(\\beta_{t-1}+\\alpha_{t-1}(\u0026hellip;(\\beta_2+\\alpha_2(\\beta_1))))\\\\ =\\beta_t+\\alpha_t(\\beta_{t-1}+\\alpha_{t-1}(\u0026hellip;(\\beta_2+\\alpha_2(\\beta_1+\\alpha_1)))) $$ 如果令： $$ \\alpha_i+\\beta_i=1,\\forall i=1,\u0026hellip;t $$ 则\n$$ \\bar \\alpha_t+\\bar\\beta_t=1 $$ 所以： $$ q(x_t|x_{t-1})=\\mathcal N(x_t;\\sqrt{1-\\beta_t}x_{t-1},\\beta_t\\bold I)\\\\ q(x_t|x_{0})=\\mathcal N(x_t;\\sqrt{\\bar \\alpha_t}x_{0},(1-\\bar\\alpha_t)\\bold I)\\\\ $$\n逆向过程 去噪，生成。\nVAE涉及一步编码+解码，涉及三个分布，编码器$q(z|x)$，解码器$p(x|z)$和隐空间分布$q(z)$。 $$ x\\to z\\\\ z\\to x $$\n而扩散模型可以看成多个VAE模型（Hierarchical Variational Auto-Encoder）。类似用分段函数逼近曲线。 $$ x_0\\to x_1 \\to x_2 \\to \u0026hellip; \\to x_T\\\\ x_T \\to x_{T-1} \\to x_{T-2} $$ 如果每一步逆向过程为$p_\\theta (x_{t-1}|x_t)$： $$ p_\\theta(x_{t-1}|x_t):= \\mathcal N(x_{t-1};\\mu_\\theta(x_t,t),\\Sigma_\\theta(x_t,t))\\\\ p_\\theta(x_{0:T})=p_\\theta(x_0,x_1,\u0026hellip;,x_T)=p(x_T)\\prod_{t=1}^{T}p_{\\theta}(x_{t-1}|x_t) $$\n损失函数 下面插两条回忆，可以略过：\n回忆1：KL散度公式 $$ \\begin{aligned} D_{KL}(P||Q)\u0026amp;=\\mathbb E_{x\\sim P}\\log\\frac{P}{Q}\\\\ \u0026amp;=\\mathbb E_{x\\sim P}-\\log\\frac{Q}{P}\\\\ \u0026amp;\\ge-\\log \\mathbb E_{x\\sim P} \\frac{Q}{P}\\\\ \u0026amp;=-\\log\\int_{P(x)\\neq 0}\\frac{QP}{P}dx\\\\ \u0026amp;\\ge -\\log 1=0 \\end{aligned} $$ 回忆2：Evidence Lower Bound, ELBO\n$x$的log证据（evidence）$\\log p_\\theta(x)$，根据 Jensen 不等式有： $$ \\begin{aligned} \\log p_\\theta(x)\u0026amp;=\\log \\int p_\\theta(x,z)dz\\\\ \u0026amp;=\\log \\mathbb E_{z\\sim q_\\phi(z|x)} \\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\\\ \u0026amp;\\ge \\mathbb E_{z\\sim q_\\phi(z|x)} \\log \\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\\\ \\end{aligned} $$ 因此我们计$L(\\phi,\\theta;x)=\\mathbb E_{z\\sim q_\\phi(z|x)} \\log \\frac{p_\\theta(x,z)}{q_\\phi(z|x)}$为evidence lower bound，ELBO。evidence与ELBO之间正好差一个KL散度。 $$ \\begin{aligned} \\log p_\\theta(x)-\\mathbb E_{z\\sim q_\\phi(z|x)} \\log \\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\u0026amp;=\\log \\frac{p_\\theta(x,z)}{p_\\theta(z|x)}-\\mathbb E_{z\\sim q_\\phi(z|x)} \\log \\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\\\ \u0026amp;=\\mathbb E_{z\\sim q_\\phi(z|x)} \\log \\frac{p_\\theta(x,z)}{p_\\theta(z|x)}-\\mathbb E_{z\\sim q_\\phi(z|x)} \\log \\frac{p_\\theta(x,z)}{q_\\phi(z|x)}\\\\ \u0026amp;=\\mathbb E_{z\\sim q_\\phi(z|x)} \\log \\frac{q_\\phi(z|x)}{p_\\theta(z|x)}\\\\ \u0026amp;=D_{KL}(q_\\phi(z|x)||p_\\theta(z|x))\\ge 0 \\end{aligned} $$\n回到正题，我们需要最小化负对数似然的变分上界（the usual variational bound on negative log likelihood），并将此定义为$L$:\n$$ \\begin{aligned} \\mathbb E[-\\log p_\\theta (x_0)]= \u0026amp;\\mathbb E\\bigg[-\\log \\frac{p_\\theta (x_{0:T})q(x_{1:T}|x_0)}{p_\\theta(x_{1:T}|x_0)q(x_{1:T}|x_0)}\\bigg]\\\\ =\u0026amp;\\mathbb E_{q(x_{1:T}|x_0)}\\bigg[-\\log \\frac{p_\\theta (x_{0:T})}{q(x_{1:T}|x_0)}\\bigg]-D_{KL}(q(x_{1:T}|x_0)||p_\\theta(x_{1:T}|x_0))\\\\ \\le\u0026amp;\\mathbb E_{q}\\bigg[-\\log \\frac{p_\\theta (x_{0:T})}{q(x_{1:T}|x_0)}\\bigg]\\\\ =\u0026amp;\\mathbb E_{q}\\bigg[-\\log p(x_T) -\\sum_{t\\ge 1}\\log \\frac{p_\\theta (x_{t-1}|x_{t})}{q(x_{t}|x_{t-1})}\\bigg]:=L\\\\ \\end{aligned} $$\n由于扩散过程遵循马尔可夫链，所以： $$ \\begin{aligned} q(x_t|x_{t-1})=\u0026amp;q(x_t|x_{t-1},x_0)\\\\ =\u0026amp;\\frac{q(x_t,x_{t-1},x_0)}{q(x_{t-1},x_0)}\\\\ =\u0026amp;\\frac{q(x_{t-1}|x_t,x_0)q(x_t|x_0)q(x_0)}{q(x_{t-1}|x_0)q(x_0)}\\\\ =\u0026amp;q(x_{t-1}|x_t,x_0)\\frac{q(x_t|x_0)}{q(x_{t-1}|x_0)}(t\u0026gt;1) \\end{aligned} $$\n我们可以重写$L$\n$$ \\begin{aligned} L=\u0026amp;\\mathbb E_{q}\\bigg[-\\log \\frac{p_\\theta (x_{0:T})}{q(x_{1:T}|x_0)}\\bigg]\\\\ =\u0026amp;\\mathbb E_{q}\\bigg[-\\log p(x_T) -\\sum_{t\\ge 1}\\log \\frac{p_\\theta (x_{t-1}|x_{t})}{q(x_{t}|x_{t-1})}\\bigg]\\\\ =\u0026amp;\\mathbb E_{q}\\bigg[-\\log p(x_T) +\\sum_{t\u0026gt; 1}\\log \\frac{q(x_{t-1}|x_{t},x_0)}{p_\\theta (x_{t-1}|x_{t})}\\frac{q(x_t|x_0)}{q(x_{t-1}|x_0)}-\\log\\frac{p_\\theta(x_0|x_1)}{q(x_1|x_0)}\\bigg]\\\\ =\u0026amp;\\mathbb E_q \\bigg[\\log\\frac{q(x_T|x_0)}{p(x_T)}+\\sum_{t\\gt 1}\\log\\frac{q(x_{t-1}|x_{t},x_0)}{p_\\theta (x_{t-1}|x_{t})} -\\log p_\\theta(x_0|x_1) \\bigg]\\\\ =\u0026amp;\\mathbb E_q\\bigg[\\underbrace{D_{KL}(q(x_T|x_0)||p(x_T))}_{L_T} + \\sum_{t\u0026gt;1} \\underbrace{D_{KL}(q(x_{t-1}|x_{t},x_0)||p_\\theta(x_{t-1}|x_t))}_{L{t-1}} \\underbrace{-\\log p_\\theta(x_0|x_1)}_{L_0} \\bigg] \\end{aligned} $$\n这时用KL散度将$p_\\theta(x_{t-1}|x_t)$与前向过程的后验概率$q(x_{t-1}|x_t,x_0)$相比。并且$q(x_{t-1}|x_t,x_0)$是可以写出解析形式的。\n$$ \\begin{aligned} \u0026amp;q(x_{t-1}|x_t,x_0)\\\\ =\u0026amp;\\frac{q(x_{t-1},x_{t}|x_{0})}{q(x_t|x_0)}\\\\ =\u0026amp;\\frac{q(x_t|x_{t-1})q(x_{t-1}|x_{0})}{q(x_t|x_0)}\\\\ =\u0026amp;\\frac{\\mathcal N(x_{t};\\sqrt{\\alpha_t}x_{t-1},\\beta_t\\bold I)\\mathcal N(x_{t-1};\\sqrt{\\bar \\alpha_{t-1}}x_0,\\bar \\beta_{t-1}\\bold I)}{\\mathcal N(x_t;\\sqrt{\\bar \\alpha_{t}}x_0,\\bar \\beta_{t}\\bold I)}\\\\ \\propto \u0026amp; \\exp\\bigg[\\bigg(-\\frac{(x_t-\\sqrt{\\alpha_t}x_{t-1})^2}{1-\\alpha_t}- \\frac{(x_{t-1}-\\sqrt{\\bar\\alpha_{t-1}}x_{0})^2}{1-\\bar\\alpha_{t-1}}+\\frac{(x_t-\\sqrt{\\bar\\alpha_t}x_{0})^2}{1-\\bar\\alpha_t}\\bigg)\\times\\frac{1}{2}\\bigg]\\\\ =\u0026amp;\\exp\\bigg[\\bigg(-\\frac{\\alpha_tx_{t-1}^2-2\\sqrt{\\alpha_t}x_{t-1}x_t}{1-\\alpha_t}- \\frac{x_{t-1}^2-2\\sqrt{\\bar\\alpha_{t-1}}x_{0}x_{t-1}}{1-\\bar\\alpha_{t-1}}+C(x_t,x_t)\\bigg)\\times\\frac{1}{2}\\bigg]\\\\ =\u0026amp;\\exp\\bigg[-\\bigg(\\frac{(\\alpha_t\\bar\\beta_{t-1}+\\beta_t)x_{t-1}^2-2(\\sqrt{\\alpha_t}\\bar\\beta_{t-1}x_t+\\sqrt{\\bar\\alpha_{t-1}}\\beta_t x_0)x_{t-1}}{2\\beta_t\\bar\\beta_{t-1}}\\bigg)+C(x_0,x_t)\\bigg]\\\\ =\u0026amp;\\exp\\bigg[-\\frac{x_{t-1}^2-2\\frac{\\sqrt{\\alpha_t}\\bar\\beta_{t-1}x_t+\\sqrt{\\bar\\alpha_{t-1}}\\beta_t x_0}{\\alpha_t\\bar\\beta_{t-1}+\\beta_t}x_{t-1}}{2\\frac{\\beta_t\\bar\\beta_{t-1}}{\\alpha_t\\bar\\beta_{t-1}+\\beta_t}}+C(x_0,x_t)\\bigg]\\\\ =\u0026amp;\\exp\\bigg[-\\frac{\\big(x_{t-1}-\\frac{\\sqrt{\\alpha_t}\\bar\\beta_{t-1}x_t+\\sqrt{\\bar\\alpha_{t-1}}\\beta_t x_0}{\\alpha_t\\bar\\beta_{t-1}+\\beta_t}\\big)^2}{2\\frac{\\beta_t\\bar\\beta_{t-1}}{\\alpha_t\\bar\\beta_{t-1}+\\beta_t}}+C(x_0,x_t)\\bigg]\\\\ =\u0026amp;\\exp\\bigg[-\\frac{\\big(x_{t-1}-\\frac{\\sqrt{\\alpha_t}\\bar\\beta_{t-1}x_t+\\sqrt{\\bar\\alpha_{t-1}}\\beta_t x_0}{\\bar\\beta_{t}}\\big)^2}{2\\frac{\\beta_t\\bar\\beta_{t-1}}{\\bar\\beta_{t}}}+C(x_0,x_t)\\bigg] \\end{aligned} $$\n所以 $$ q(x_{t-1}|x_t,x_0)=\\mathcal N(x_{t-1};\\tilde\\mu_t(x_t,x_0),\\tilde\\beta_t\\bold I) $$ 其中： $$ \\tilde\\mu_t(x_t,x_0):=\\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}x_t+\\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1-\\bar\\alpha_t}x_0\\\\ \\tilde\\beta_t:=\\frac{1-\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t}\\beta_t $$\n对$L$进行分类讨论：\n由于固定了$\\beta_t$，所以$p(x_T)$和$q(x_T|x_0)$没有可学习的参数，因此$L_T$可以忽略\n$L_{1:T-1}$ 由于$p_\\theta(x_{t-1}|x_t)=\\mathcal N(x_{t-1};\\mu_\\theta(x_t,t),\\Sigma_\\theta(x_t,t)),1\\lt t\\le T$，我们令$\\Sigma_\\theta(x_t,t)=\\sigma_t^2\\bold I$是一个和$\\beta_t$有关的固定的量，比如$\\sigma_t^2=\\beta_t$或$\\sigma_t^2=\\tilde\\beta_t$。所以$p_\\theta(x_{t-1}|x_t)$只有$\\mu_\\theta(x_t,t)$需要学习。 $$ p_\\theta(x_{t-1}|x_t)=\\mathcal N(x_{t-1};\\mu_\\theta(x_t,t),\\sigma_t^2\\bold I) $$ 因为两个正态分布$P=\\mathcal N(\\mu_1,\\sigma_1^2)$和$Q=\\mathcal N(\\mu_2,\\sigma_2^2)$的KL散度为\n$$ D(P||Q)=\\int \\bigg[\\log \\frac{\\sigma_2}{\\sigma_1}+\\frac{\\sigma_1^2(x-\\mu_2)^2-\\sigma_2^2(x-\\mu_1)^2}{2\\sigma_1^2\\sigma_2^2}\\bigg]\\frac{1}{\\sqrt{2\\pi\\sigma_1^2}}\\exp\\bigg(-\\frac{(x-\\mu_1)^2}{2\\sigma_1^2}\\bigg)\\text dx\\\\ =\\log \\frac{\\sigma_2}{\\sigma_1}+\\frac{1}{\\sqrt{\\pi}}\\int \\bigg(\\frac{\\sigma_1^2+\\sigma_2^2}{\\sigma_2^2}x^2+\\frac{(\\mu_1-\\mu_2)^2}{2\\sigma_2^2}\\bigg)\\exp(-x^2)\\text dx\\\\ =\\log \\frac{\\sigma_2}{\\sigma_1}+\\frac{\\sigma_1^2+\\sigma_2^2}{2\\sigma_2^2}+\\frac{(\\mu_1-\\mu_2)^2}{2\\sigma_2^2} $$\n所以\n$$ \\begin{aligned} L_{t-1}=\u0026amp;\\mathbb E_q \\big[D_{KL}(q(x_{t-1}|x_t,x_0)||p_\\theta(x_{t-1}|x_t))\\big]\\\\ =\u0026amp;C+\\mathbb E_q \\bigg[\\frac{||\\tilde\\mu_t(x_t,x_0)-\\mu_\\theta(x_t,t)||^2}{2\\sigma_t^2}\\bigg]\\\\ =\u0026amp;C+\\mathbb E_{x_0,\\epsilon}\\bigg[\\frac{1}{2\\sigma_t^2}\\bigg\\Vert \\tilde \\mu_t\\bigg(x_t(x_0,\\epsilon),\\frac{1}{\\sqrt{\\bar \\alpha_t}}(x_t(x_0,\\epsilon)-\\sqrt{1-\\bar\\alpha_t}\\epsilon)\\bigg)-\\mu_\\theta(x_t(x_0,t),t)\\bigg\\Vert^2\\bigg] \\end{aligned} $$\n将$\\tilde\\mu_t(x_t,x_0)$化简，得\n$$ \\begin{aligned} \\tilde\\mu_t(x_t,x_0):=\u0026amp;\\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}x_t+\\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1-\\bar\\alpha_t}x_0\\\\ =\u0026amp;\\bigg(\\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}+\\frac{\\beta_t}{\\sqrt{\\alpha_t}(1-\\bar\\alpha_t)}\\bigg)x_t(x_0,\\epsilon)-\\frac{\\beta_t}{\\sqrt{\\alpha_t}\\sqrt{1-\\bar\\alpha_t}}\\epsilon\\\\ =\u0026amp;\\frac{1}{\\sqrt{\\alpha_t}}\\bigg(x_t(x_0,\\epsilon)-\\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\epsilon\\bigg) \\end{aligned} $$\n代入$L_{t-1}$得： $$ \\begin{aligned} L_{t-1}=\u0026amp;C+\\mathbb E_{x_0,\\epsilon}\\bigg[\\frac{1}{2\\sigma_t^2}\\bigg\\Vert \\tilde \\mu_t\\bigg(x_t(x_0,\\epsilon),\\frac{1}{\\sqrt{\\bar \\alpha_t}}(x_t(x_0,\\epsilon)-\\sqrt{1-\\bar\\alpha_t}\\epsilon)\\bigg)-\\mu_\\theta(x_t(x_0,t),t)\\bigg\\Vert^2\\bigg]\\\\ =\u0026amp;C+\\mathbb E_{x_0,\\epsilon}\\bigg[ \\frac{1}{2\\sigma_t^2}\\bigg\\Vert \\frac{1}{\\sqrt{\\alpha_t}}\\bigg(x_t(x_0,\\epsilon)-\\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\epsilon\\bigg)-\\mu_\\theta(x_t(x_0,t),t)\\bigg\\Vert^2\\bigg] \\end{aligned} $$ 因此可以看出，我们需要用$\\mu_\\theta$来预测给定$x_t$时的$\\frac{1}{\\sqrt{\\alpha_t}}(x_t-\\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\epsilon)$。因为$x_t$是模型的输入，我们可以选择如下的参数形式 $$ \\mu_\\theta(x_t,t)=\\tilde \\mu_t\\bigg(x_t,\\frac{1}{\\sqrt{\\bar\\alpha_t}}(x_t-\\sqrt{1-\\bar\\alpha_t}\\epsilon_\\theta(x_t,t))\\bigg)=\\frac{1}{\\sqrt{\\alpha_t}}\\bigg(x_t-\\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\epsilon_\\theta(x_t,t)\\bigg) $$ 其中$\\epsilon_\\theta(x_t,t)$就是从$x_t$预测$\\epsilon$，采样过程就是采样$x_{t-1}\\sim p_\\theta(x_{t-1}|x_t)=\\mathcal N(x_{t-1};\\mu_\\theta(x_t,t),\\sigma_t^2\\bold I)$\n这时，$L_{t-1}-C$就可以简写成： $$ L_{t-1}-C=\\mathbb E_{x_0,\\epsilon}\\bigg[\\frac{\\beta_t^2}{2\\sigma_t^2\\alpha_t(1-\\bar\\alpha_t)}\\bigg\\Vert\\epsilon-\\epsilon_\\theta(x_t,t)\\bigg\\Vert^2\\bigg]\\\\ =\\mathbb E_{x_0,\\epsilon}\\bigg[\\frac{\\beta_t^2}{2\\sigma_t^2\\alpha_t(1-\\bar\\alpha_t)}\\bigg\\Vert\\epsilon-\\epsilon_\\theta(\\sqrt{\\bar\\alpha_t}x_0+\\sqrt{1-\\bar\\alpha_t}\\epsilon,t)\\bigg\\Vert^2\\bigg] $$ 综上所述，训练和采样过程就出来了：\n模型训练（左）和采样（右）\r参考 一文解释 Diffusion Model (一) 理论推导 Denoising Diffusion Probabilistic Models ","permalink":"https://xiergo.github.io/posts/tech/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/","summary":"扩散过程/前向过程（从右到左）逆扩散过程/重建过程（从左到右） 前向过程 前向过程是一个加噪音的过程： $$ q(x_t|x_{t-1}):=\\mathcal N(x_t;\\sqrt{\\alpha_t}x_{t-1},\\beta_t\\bold I) $$ 即： $$ x_t=\\sqrt{\\alpha_t}x_{t-1}+\\sqrt{\\beta_t}\\epsilon_t $$ 其中，$\\epsilon_t\\sim\\mathcal N(0,I)$是噪音，$\\sqrt{\\beta_t}\\in(0,1)$是噪声因子，$\\sqrt{\\alpha_","title":"扩散模型"},{"content":"\r度量（Metric） $M$是一个集合，我们称$d:M\\times M \\rightarrow \\R $ 是 $M$上的度量，，如果它满足以下条件：\n$d(x,x)=0$ 正定性：$d(x,y)\\ge 0$当且仅当$x=y$等号成立 对称性：$d(x,y)=d(y,x)$ 三角不等式：$d(x,y)\\le d(x,z)+d(y,z)$ 这是，我们称$(M,d)$是一个度量空间。\nKL散度 KL散度（Kullback-Leibler divergence）也叫相对熵（relative entropy）或者I散度（I-divergence），记作\n$$ D_{KL}(P||Q) $$\n它是一种统计距离，度量随机分布$P$和另一参考的随机分布$Q$之间的距离。换一句话说，$P$对$Q$的KL散度就是当真实分布是$P$时，我们用$Q$来统计结果得到的惊讶程度的期望。KL散度没有对称性和三角不等式，因此是距离但不是度量。\n$$ \\begin{aligned} D_{KL}(P||Q)\u0026amp;=\\sum_{x \\in \\mathcal X} P(x)\\log \\frac{P(x)}{Q(x)}\\\\ \u0026amp;=-\\sum_{x \\in \\mathcal X} P(x)\\log\\frac{Q(x)}{P(X)}\\\\ \u0026amp;=\\mathbb E_{x\\sim P(x)}\\bigg [-\\log\\frac{Q(x)}{P(X)}\\bigg ] \\end{aligned} $$\n熵 $$ H(P)=\\sum P(x)\\log\\frac{1}{P(x)} $$\n交叉熵 （Cross entropy） $$ H(P,Q)=\\sum P(x)\\log \\frac{1}{Q(x)} $$\n所以KL散度： $$ \\begin{aligned} D_{KL}(P,Q)\u0026amp;=\\sum P(x) \\log \\frac{P(x)}{Q(x)}\\\\ \u0026amp;=H(P,Q)-H(P) \\end{aligned} $$\n证明： KL散度$D_{KL}(P||Q)\\ge0$\n证： $$ \\begin{aligned} D_{KL}(P,Q)\u0026amp;=\\sum P(x) \\log \\frac{P(x)}{Q(x)}\\\\ \u0026amp;=\\mathbb E_{x\\sim P(x)} \\bigg[-\\log\\frac{Q(x)}{P(x)}\\bigg]\\\\ \u0026amp;\\ge -\\log \\bigg( \\mathbb E_{x\\sim P(x)}\\bigg[\\frac{Q(x)}{P(x)}\\bigg]\\bigg) \\\\ \u0026amp;=-\\log(\\sum_{P(x)\\neq0} Q(x))\\ge-\\log(1)=0 \\end{aligned} \\tag{Jensen不等式} $$\n例 已知$P\\sim \\mathcal N(\\mu_1,\\sigma_1^2),Q\\sim\\mathcal N(\\mu_2,\\sigma_2^2)$，求解$D_{KL}(P||Q)$\n解： $$ \\begin{aligned} D_{KL}(P||Q)=\u0026amp;\\mathbb E_{P(x)} \\log\\frac{P(x)}{Q(x)}\\\\ =\u0026amp;\\int \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}}\\exp\\bigg({-\\frac{(x-\\mu_1)^2}{2\\sigma_1^2}}\\bigg)\\bigg[\\frac{\\sigma_1^2(x-\\mu_2)^2-\\sigma_2^2(x-\\mu_1)^2}{2\\sigma_1^2\\sigma_2^2}+\\log\\frac{\\sigma_2}{\\sigma_1}\\bigg]\\text dx\\\\ =\u0026amp;\\frac{\\sqrt{2\\sigma_1^2}}{\\sqrt{2\\pi\\sigma_1^2}}\\int \\frac{\\sigma_1^2(\\sqrt{2}\\sigma_1 x+\\mu_1-\\mu_2)^2-2\\sigma_1^2\\sigma_2^2x^2}{2\\sigma_1^2\\sigma_2^2}\\exp(-x^2)\\text dx+\\log\\frac{\\sigma_2}{\\sigma_1}\\\\ =\u0026amp;\\frac{1}{\\sqrt{\\pi}}\\int \\bigg[\\frac{\\sigma_1^2-\\sigma_2^2}{\\sigma_2^2}x^2 +\\frac{(\\mu_1-\\mu_2)^2}{2\\sigma_2^2}\\bigg]\\exp(-x^2)\\text dx+\\log\\frac{\\sigma_2}{\\sigma_1}\\\\ =\u0026amp;\\frac{\\sigma_1^2-\\sigma_2^2}{2\\sigma_2^2}+\\frac{(\\mu_1-\\mu_2)^2}{2\\sigma_2^2}+\\log\\frac{\\sigma_2}{\\sigma_1} \\end{aligned} $$\n事实上，由高斯积分可得：$\\int\\exp(-x^2)=\\sqrt{\\pi}, \\int x^2 \\exp(-x^2)=\\sqrt{\\pi}/2$，证明如下： $$ \\begin{aligned} \\int_0^{+\\infin}\\exp(-x^2)dx=\u0026amp;\\sqrt{\\int_0^{+\\infin}\\exp(-x^2)\\text dx\\int_0^{+\\infin}\\exp(-y^2)\\text dy}\\\\ =\u0026amp;\\sqrt{\\int_0^{+\\infin}\\int_0^{+\\infin}\\exp(-x^2-y^2)\\text dx\\text dy}\\\\ =\u0026amp;\\sqrt{\\int_0^{\\frac{\\pi}{2}}\\int_0^{+\\infin}\\exp(-r^2)r\\text dr\\text d\\theta}\\\\ =\u0026amp;\\sqrt{\\frac{\\pi}{4}} \\end{aligned} $$\n$$ \\begin{aligned} \\int_{-\\infin}^{\\infin} x^2\\exp(-x^2)dx=\u0026amp;-\\frac{1}{2}\\int_{-\\infin}^{\\infin}x\\text d\\exp(-x^2)\\\\ =\u0026amp;-\\frac{1}{2}x\\exp(-x^2)\\bigg \\vert_{-\\infin}^{\\infin}+\\frac{1}{2}\\int_{-\\infin}^{\\infin}\\exp(-x^2)\\text dx\\\\ =\u0026amp;\\frac{\\sqrt{\\pi}}{2} \\end{aligned} $$\n","permalink":"https://xiergo.github.io/posts/tech/kl%E6%95%A3%E5%BA%A6/","summary":"度量（Metric） $M$是一个集合，我们称$d:M\\times M \\rightarrow \\R $ 是 $M$上的度量，，如果它满足以下条件： $d(x,x)=0$ 正定性：$d(x,y)\\ge 0$当且仅当$x=y$等号成立 对称性：$d(x,y)=d(y,x)$ 三角不等式：$d(x,y)\\le d(x,z)+d(y,z)$ 这是，我们称$(M,d)$是一个度量","title":"KL散度"},{"content":"\r最大似然估计(Maximum likelihood estimation, MLE) 有一些样本$X$，我们假设它是从一种分布（比如正太分布）中产生的数据，但是分布的参数$\\theta$我们并不知道，我们希望找到这样的$\\theta$:\n$$ \\begin{aligned}\\hat \\theta\u0026amp;=\\underset{\\theta}{\\operatorname{argmax}} P(\\theta|X)\\\\\u0026amp;=\\underset{\\theta}{\\operatorname{argmax}} \\frac{P(\\theta ;X)}{P(X)}\\\\\u0026amp;=\\underset{\\theta}{\\operatorname{argmax}} \\frac{P(X|\\theta )P(\\theta)}{P(X)}\\\\ \u0026amp;=\\underset{\\theta}{\\operatorname{argmax}}P(X|\\theta )P(\\theta) \\end{aligned} \\tag{1} $$\n$X$的分布$P(X)=\\int_\\Theta P(X|\\theta)P(\\theta)\\text d\\theta$是不可知的，但这对我们选最好的$\\theta$没有影响，故可以去掉。\n这里$P(X|\\theta )P(\\theta)$就是后验概率（posterior）的分子（A posteriori）,直接最大化后验概率的方法就是最大后验概率估计（Maximum a posteriori estimation, MAP）。\n$$ \\begin{aligned} \\hat \\theta_{MLE}=\u0026amp;\\underset{\\theta}{\\operatorname{argmax}} P(X|\\theta)P(\\theta)\\\\ =\u0026amp;\\underset{\\theta}{\\operatorname{argmax}} P(\\theta) \\prod_{i=1}^n P(x_i|\\theta)\\\\ =\u0026amp;\\underset{\\theta}{\\operatorname{argmin}} -\\operatorname{log}(P(\\theta))-\\sum_{i=1}^n \\operatorname{log}(P(x_i|\\theta)) \\end{aligned} \\tag{2} $$\n如果，$\\theta$的分布也不知道，我们没有任何关于$\\theta$的先验知识，不知道哪个$\\theta$更好，因此，我们使用uninformative prior，即认为$\\theta$在其参数空间是均匀分布的，所以$P(\\theta)$是一个常数，也可以去掉。 $$ \\begin{aligned} \\hat \\theta_{MLE}=\u0026amp;\\underset{\\theta}{\\operatorname{argmax}} P(X|\\theta)\\\\ =\u0026amp;\\underset{\\theta}{\\operatorname{argmax}} \\prod_{i=1}^n P(x_i|\\theta)\\\\ =\u0026amp;\\underset{\\theta}{\\operatorname{argmin}} -\\sum_{i=1}^n \\operatorname{log}(P(x_i|\\theta)) \\end{aligned} \\tag{3} $$ 其中$P(X|\\theta)$就是似然（likelihood）。\n均方误差（Mean squared error, MSE） 现有一些样本点${(x_i, y_i)|i=1,\u0026hellip;,n}$，对于模型$f(x)$的MSE就是： $$ \\text {MSE}=1 / n\\sum_{i=1}^n (f(x_i|\\theta)-y_i)^2 $$ 本质上来说，这就是MLE。\n假设$y_i \\sim \\mathcal N(f(x_i|\\theta),\\sigma^2) $，则似然函数就是 $$ \\ell(\\theta)=\\prod_{i} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\bigg({-\\frac{(f(x_i|\\theta)-y_i)^2}{2\\sigma^2}}\\bigg) $$ 最大似然就相当于最小化$-\\log\\ell(\\theta)$ $$ \\begin{aligned} \\hat\\theta=\u0026amp;\\underset{\\theta}{\\operatorname{argmin}}-\\log \\ell(\\theta)\\\\ =\u0026amp;\\underset{\\theta}{\\operatorname{argmin}} \\frac{n}{2}\\log(2\\pi\\sigma^2)+\\frac{1}{2\\sigma^2}\\sum_i (f(x_i|\\theta)-y_i)^2\\\\ =\u0026amp;\\underset{\\theta}{\\operatorname{argmin}} \\frac{1}{n}\\sum_{i=1}^n (f(x_i|\\theta)-y_i)^2\\\\ =\u0026amp;\\underset{\\theta}{\\operatorname{argmin}} \\text{MSE} \\end{aligned} $$\n","permalink":"https://xiergo.github.io/posts/tech/mle/","summary":"最大似然估计(Maximum likelihood estimation, MLE) 有一些样本$X$，我们假设它是从一种分布（比如正太分布）中产生的数据，但是分布的参数$\\theta$我们并不知道，我们希望找到这样的$\\theta$: $$ \\begin{aligned}\\hat \\theta\u0026amp;=\\underset{\\theta}{\\operatorname{argmax}} P(\\theta|X)\\\\\u0026amp;=\\underset{\\theta}{\\operatorname{argmax}} \\frac{P(\\theta ;X)}{P(X)}\\\\\u0026amp;=\\underset{\\theta}{\\operatorname{argmax}} \\frac{P(X|\\theta )P(\\theta)}{P(X)}\\\\ \u0026amp;=\\underset{\\theta}{\\operatorname{argmax}}P(X|\\theta )P(\\theta) \\end{aligned} \\tag{1} $$ $X$的分布$P(X)=\\int_\\Theta P(X|\\theta)P(\\theta)\\text d\\theta$是不可","title":"MLE"},{"content":"\r自编码器（Autoencoder, AE） 自编码器，利用神经网络进行数据将维，类似非线性PCA。 $$ z=g(x)\\\\ \\tilde{x}=f(z)\\\\ \\ell=||x-\\tilde x||^2 $$\nAutoencoder\r变分自编码器（Variational autoencoder, VAE） 模型接受输入$x$，经过encoder压缩到一个隐空间（latent space），从隐空间采样$z$，并将$z$经过decoder得到$\\tilde X$，希望产生的$\\tilde x$尽可能接近$x$。\n我们想最大化似然函数$p(x|\\theta)$，比如高斯分布$\\mathcal N(x|\\mu,\\sigma^2)$ $$ \\begin{aligned} p_\\theta(x)\u0026amp;=\\int_zp_\\theta(x,z)dz\\\\ \u0026amp;=\\int_zp_\\theta(x|z)p_\\theta(z)dz \\end{aligned} $$ 不太好积分，因此，我们想找一个$q_\\phi(z|x)\\approx p_\\theta(z|x)$\nKL散度（Kullback–Leibler divergence） Evidence lower bound\r$$ \\begin{aligned} D_{KL}(q_\\phi(z|x)||p_\\theta(z|x))\u0026amp;=\\mathbb E_{z\\sim q_\\phi(.|x)}\\bigg[\\log\\frac{q_\\phi(z|x)}{p_\\theta(z|x)}\\bigg]\\\\ \u0026amp;=\\mathbb E_{z\\sim q_\\phi(.|x)}\\bigg[\\log\\frac{q_\\phi(z|x)p_\\theta(x)}{p_\\theta(z|x)p_\\theta(x)}\\bigg]\\\\ \u0026amp;=\\mathbb E_{z\\sim q_\\phi(.|x)}\\bigg[\\log\\frac{q_\\phi(z|x)p_\\theta(x)}{p_\\theta(z,x)}\\bigg]\\\\ \u0026amp;=\\log p_\\theta(x)+\\mathbb E_{z\\sim q_\\phi(.|x)}\\bigg[\\log\\frac{q_\\phi(z|x)}{p_\\theta(z,x)}\\bigg]\\\\ \\end{aligned} $$ 这里定义evidence lower bound (ELBO) $$ L_{\\phi,\\theta}(x):=\\mathbb E_{z\\sim q_\\phi(z|x)}\\bigg[\\log \\frac{p_\\theta(z,x)}{q_\\phi(z|x)}\\bigg]\\\\ =\\log p_\\theta(x)-D_{KL}(q_\\phi(z|x)||p_\\theta(z|x)) $$ 最大化ELBO就相当于最大化观测数据的log-likelihood，同时最小化估计的posterior （$q_\\phi(z|x)$）和真实的posterior ($p_\\theta(z|x)$)之间的差距。\n","permalink":"https://xiergo.github.io/posts/tech/vae/","summary":"自编码器（Autoencoder, AE） 自编码器，利用神经网络进行数据将维，类似非线性PCA。 $$ z=g(x)\\\\ \\tilde{x}=f(z)\\\\ \\ell=||x-\\tilde x||^2 $$ Autoencoder 变分自编码器（Variational autoencoder, VAE） 模型接受输入$x$，经过encoder压缩到一个隐空间（latent space），从隐空间采样$z$，并将$z$经过decod","title":"VAE"},{"content":"\r上周报名了昌平实验室实习，主要工作就是文献调研、公司调研和PI调研。有钱可以拿，还能迫使自己多读文献，减少自己游戏时间，一举两得。\n我是最后一个面试，自己的简历还ok，至少从面试老师给我的感觉来看，我应该是参加面试比较优秀的了——属于矮子里面挑将军。面试的整理还算顺利，老师对我都印象蛮好，我也是毫无悬念的通过了这次面试。但是面试的过程还是有一些小的波折，自己还是会情不自禁地紧张。记得上一次实习面试biomap的时候，苏哲学长问我导师是谁，我一紧张竟然忘记了导师的名字。我需要多锻炼自己的这方面能力，减少演讲、面试等场合的紧张情绪。\n","permalink":"https://xiergo.github.io/posts/life/%E9%9D%A2%E8%AF%95%E6%98%8C%E5%B9%B3%E5%AE%9E%E9%AA%8C%E5%AE%A4%E5%AE%9E%E4%B9%A0/","summary":"上周报名了昌平实验室实习，主要工作就是文献调研、公司调研和PI调研。有钱可以拿，还能迫使自己多读文献，减少自己游戏时间，一举两得。 我是最后一个面试，自己的简历还ok，至少从面试老师给我的感觉来看，我应该是参加面试比较优秀的了——属于矮子里面挑将军。面试的整理还算顺利，老师对我都印","title":"面试昌平实验室实习"},{"content":"\r抗体简介 抗体根据其重链恒定区不同，可以分为五类，如下表：\n抗体分类\r以IgG为例，抗体由2条重链和两条轻链组成。其中，重链由14号染色体编码。轻链有两种，$\\kappa$链由2号染色体编码，$\\lambda$链由22号染色体编码。\nChain Chr V D J heavy chain 14 56 23 6 $\\kappa$ light chain 2 52 0 5 $\\lambda$ light chain 22 30 0 7 抗体轻重链\rV(D)J基因片段重排 如下图所示，VDJ分别为Variable (V), Diversity (D) 和 Joining (J)的首字母缩写，代表各类的基因片段。重链先DJ重排，再V和DJ重排。轻链只有VJ重排。RSS(Recombination Signal Sequences)保证了VDJ排列的顺序是对的，包括12RSS、23RSS，序列长度不一样。Constant区域相对固定，重链有3个恒定去区，一个可变区，轻链有一个恒定区，一个可变区。可变区包括4个框架区（Framework Region, FR）和3个互补决定区（complementary determing region, CDR），其中CDR区就是与抗原紧密接触的区域。CDR1，CDR2由V区的片段构成，CDR3由VDJ（重链）或VJ（轻链）组成，因此其多样性最高。\nV(D)J重排详细示意图。\r参考文献 Dunn-Walters D, Townsend C, Sinclair E, Stewart A. Immunoglobulin gene analysis as a tool for investigating human immune responses. Immunol Rev. 2018 Jul;284(1):132-147. doi: 10.1111/imr.12659. PMID: 29944755; PMCID: PMC6033188.\n","permalink":"https://xiergo.github.io/posts/tech/%E6%8A%97%E4%BD%93vdj%E9%87%8D%E6%8E%92/","summary":"抗体简介 抗体根据其重链恒定区不同，可以分为五类，如下表： 抗体分类 以IgG为例，抗体由2条重链和两条轻链组成。其中，重链由14号染色体编码。轻链有两种，$\\kappa$链由2号染色体编码，$\\lambda$链由22号染色体编码。 Chain Chr V D J heavy chain 14 56 23 6 $\\kappa$ light chain 2 52 0 5 $\\lambda$ light chain 22 30 0 7 抗体","title":"抗体VDJ重排"},{"content":"\r基本原理 酵母表面展示（Yeast display or yeast surface display）技术由Boder和Wittrup在1997年首创，是研究真核生物，尤其是人类蛋白质表达的理想方法之一。\n前提：酵母菌的蛋白表达方式与高等真核生物细胞十分相似\n基本原理是将外源靶蛋白基因与特定的载体基因序列融合后导入酵母细胞，利用酵母细胞的蛋白表达修饰和转运系统，将目的蛋白表达并展示在酵母细胞表面。通过流式细胞技术对酵母菌展示文库进行快速、精确的筛选，其已广泛用于：\n研究天然蛋白质－蛋白质的相互作用 抗体 Fab 片段的亲和力 抗体的检测与筛选 功能蛋白质抗原表位图谱绘制 酵母展示\r如上图，酵母表面a-凝集素由aga1p和aga2p两个核心亚基糖蛋白组成。感兴趣的蛋白被融合到Aga2p蛋白的c端，Aga2p通过两个二硫键与Aga1p共价连接，Aga1p将融合蛋白固定在细胞壁上，确保单个酵母细胞的基因型表型偶联。\n应用举例 通过酵母（saccharomyces cerevisiae）展示进行高通量新冠RBD突变的免疫逃逸扫描(deep mutational scanning, DMS)。具体做法如下：\n深度突变扫描文库构建，对Wuhan-H-1 RBD(enBank: MN908947, residues N331-T531)进行突变 利用PCR将一段特异的26核苷酸（N26）的条码（barcode）加到突变文库中的每个突变体，条码和突变体对应关系由PicBio测序得到 RBD突变体文库基因导入到EBY100酵母细胞中，并使用ACE2对表达RBD的酵母进行富集 免疫逃逸扫描，将上述的酵母细胞经过三轮的磁珠筛选，磁珠上有待测的抗体，筛选后的酵母细胞表达的就是和抗体亲和力低的RBD，也就是逃逸的RBD。 分别对筛选前(reference library, ref)、后（antibody screened, ab）的酵母细胞进行质粒提取，然后利用PCR对barcode序列进行扩增，然后测序。 突变免疫逃逸分数（mutation escape score）为 $$ \\text{escape score of each variant }X = F \\times \\frac{n_{X,ab}/N_{ab}}{n_{X,ref}/N_{ref}} $$ 其中，$n$, $N$分别代表监测出的变异体$X$的barcode和所有的barcode，ab，ref分别代表抗体筛选后的和参考文库。$F$是一个归一化因子，将分数映射到0-1范围内。然后利用上位模型（epistasis model）将每个位点的单残基突变的逃逸分数计算出来。\n参考文献 Allison J Greaney, Tyler N Starr, Jesse D Bloom, An antibody-escape estimator for mutations to the SARS-CoV-2 receptor-binding domain, Virus Evolution, Volume 8, Issue 1, 2022, veac021, https://doi.org/10.1093/ve/veac021\n","permalink":"https://xiergo.github.io/posts/tech/%E9%85%B5%E6%AF%8D%E8%A1%A8%E9%9D%A2%E5%B1%95%E7%A4%BA%E6%8A%80%E6%9C%AFyeast_display/","summary":"基本原理 酵母表面展示（Yeast display or yeast surface display）技术由Boder和Wittrup在1997年首创，是研究真核生物，尤其是人类蛋白质表达的理想方法之一。 前提：酵母菌的蛋白表达方式与高等真核生物细胞十分相似 基本原理是将外源靶蛋白基因与特定的载体基因序列融合后导入酵母细胞，利用","title":"酵母表面展示技术Yeast Display"},{"content":"今天周一，早上凌晨5点50起来送女朋友去地铁站。超级冷，路上骑车，风吹的眼睛一直在流眼泪。第一个到实验室，差不多6点40多。然后把我的书桌整理了一遍，收拾了很多陈年垃圾，瞬间清爽了很多。\n很久没有这样认真整理收拾了，感觉心情顿时舒畅了不少。又来到了最熟悉的环节：\n今天重新开始做人！！！\n","permalink":"https://xiergo.github.io/posts/life/%E5%85%83%E6%B0%94%E6%BB%A1%E6%BB%A1%E7%9A%84%E4%B8%80%E5%A4%A9/","summary":"今天周一，早上凌晨5点50起来送女朋友去地铁站。超级冷，路上骑车，风吹的眼睛一直在流眼泪。第一个到实验室，差不多6点40多。然后把我的书桌整理了一遍，收拾了很多陈年垃圾，瞬间清爽了很多。 很久没有这样认真整理收拾了，感觉心情顿时舒畅了不少。又来到了最熟悉的环节： 今天重新开始做人！！","title":"元气满满的一天"},{"content":"今天开始，记一点随笔，坚持一个记录的好习惯。折腾了半天评论的东西，结果只有开了代理才能用，不开代理就用不了，白瞎折腾了。而且github屏蔽了百度的爬虫，百度根本搜索不到任何博客的东西。既然搜不到，评不评论都无所谓了。就把这个地方当成一个记录生活、记录科研、分享技术的地方吧（分享给自己也是分享）。感谢Sulv的博客模板。\n下周六就要Work Report了，我要开始准备ppt了。烦得很！\n庆幸的是有好吃的午饭（早饭）。\n","permalink":"https://xiergo.github.io/posts/life/%E4%BB%8A%E6%97%A5%E6%9C%89%E7%82%B9%E7%83%A6/","summary":"今天开始，记一点随笔，坚持一个记录的好习惯。折腾了半天评论的东西，结果只有开了代理才能用，不开代理就用不了，白瞎折腾了。而且github屏蔽了百度的爬虫，百度根本搜索不到任何博客的东西。既然搜不到，评不评论都无所谓了。就把这个地方当成一个记录生活、记录科研、分享技术的地方吧（分享","title":"今日有点烦"},{"content":"问题描述 在本地可以部署，且正常显示 在服务器上无法显示正常格式 排除baseURL的错误 在控制台看到报integrity的错误 解决办法 将config.yml中的文件添加一个参数\n1 2 3 params: assets: disableFingerprinting: true 参考 Hugo - Failed to find a valid digest in the \u0026lsquo;integrity\u0026rsquo; attribute for resource - The resource has been blocked - Host on Github\n","permalink":"https://xiergo.github.io/posts/blog/%E4%B8%80%E4%B8%AA%E6%81%B6%E5%BF%83%E7%9A%84bug/","summary":"问题描述 在本地可以部署，且正常显示 在服务器上无法显示正常格式 排除baseURL的错误 在控制台看到报integrity的错误 解决办法 将config.yml中的文件添加一个参数 1 2 3 params: assets: disableFingerprinting: true 参考 Hugo - Failed to find a valid digest in the \u0026lsquo;integrity\u0026rsquo; attribute for resource - The resource has been blocked - Host on Github","title":"一个恶心的bug"},{"content":"需求 我们想要在paperMod中使用数学公式。\n解决办法 使用第三方JavaScript库(KaTex)\n新建一个partial文件：/layouts/partials/math.html 1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css\u0026#34; integrity=\u0026#34;sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js\u0026#34; integrity=\u0026#34;sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js\u0026#34; integrity=\u0026#34;sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; onload=\u0026#34;renderMathInElement(document.body, { delimiters: [ {left: \u0026#39;$$\u0026#39;, right: \u0026#39;$$\u0026#39;, display: true}, {left: \u0026#39;\\\\[\u0026#39;, right: \u0026#39;\\\\]\u0026#39;, display: true}, {left: \u0026#39;$\u0026#39;, right: \u0026#39;$\u0026#39;, display: false}, {left: \u0026#39;\\\\(\u0026#39;, right: \u0026#39;\\\\)\u0026#39;, display: false} ] } );\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 在/layouts/partials/extend_head.html模板文件中引入上面的math partial 1 2 3 {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} 在config.yml中的params下全局声明math: true或者在每篇博客的头部单独声明 1 2 params: math: true 效果 layout math $$ X_1=\\pi^2+x\\div3 $$\n$$ 4 \\neq5 $$\n$$ a=5 \\\\ n=3 $$\n$$ a_5=b_10 $$\ninline math\nthis is in line test: $ X_1 = \\pi ^ 2 + x \\div 3 $\n参考文章 Math Typesetting How to enable Math Typesetting in PaperMod? ","permalink":"https://xiergo.github.io/posts/blog/%E4%BD%BFpapermod%E6%94%AF%E6%8C%81%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/","summary":"","title":"使PaperMod支持数学公式"},{"content":"\rXiergo\u0026#39;s Blog\r摸鱼大师\rSulv\u0026#39;s Blog\r一个记录技术、阅读、生活的博客\r👉友链格式\r名称： Xiergo\u0026rsquo;s Blog 网址： https://xiergo.github.io 图标： https://xiergo.github.io/img/Q.jpg 描述： 摸鱼大师 👉友链申请要求\r\u003e 秉承互换友链原则、文章定期更新、不能有太多广告、个人描述字数控制在15字内\r","permalink":"https://xiergo.github.io/links/","summary":"Xiergo\u0026#39;s Blog 摸鱼大师 Sulv\u0026#39;s Blog 一个记录技术、阅读、生活的博客 👉友链格式 名称： Xiergo\u0026rsquo;s Blog 网址： https://xiergo.github.io 图标： https://xiergo.github.io/img/Q.jpg 描述： 摸鱼大师 👉友链申请要求 \u003e 秉承互换友链原则、文章定期更新、不能有太多广告、个人描述字数控制在15字内","title":"🤝友链"},{"content":"\r英文名: Xiergo 地点： 北京 爱好： 游戏，乒乓球 职业： 博士在读 关注领域： 蛋白质结构预测和序列设计、深度学习、生物信息 ","permalink":"https://xiergo.github.io/about/","summary":"英文名: Xiergo 地点： 北京 爱好： 游戏，乒乓球 职业： 博士在读 关注领域： 蛋白质结构预测和序列设计、深度学习、生物信息","title":"🙋🏻‍♂️关于"},{"content":"reference:\nGlmnet VIgnette\nIntroduction $$ min_{\\beta_0,\\beta}\\frac{1}{N}\\sum_{i=1}^{N} w_il(y_i,\\beta_0+\\beta^Tx_i)+ \\lambda[(1-\\alpha)||\\beta||_2^2/2+\\alpha||\\beta||_1]\\\\ l(y,\\eta)=(y-\\eta)^2/2 $$\n$$ y_i=\\beta_0+\\beta^Tx_i $$\nridge regression: $\\alpha=0$\nlasso regression: $\\alpha=1$\noverall strength of penalty\n","permalink":"https://xiergo.github.io/posts/tech/glmnet/","summary":"","title":"Glmnet"},{"content":"成绩 Date Reading Listening Speaking Writing Total 2018-11-14 28 19 17 18 82 2019-6-1 29 22 19 20 90 2019-11-3 29 21 17 22 89 2020-1-4 28 27 18 27 100 Best Score 29 27 19 27 102 心路历程 第一阶段的准备 确定报名密歇根项目是大三下，我开始准备考托福也差不多是大四上。大四在人民医院实习，本科基本没怎么专门学过英语，四六级也基本裸考。那时候还是说研究生第二年出去，但内心老是觉得时间还很多。这段时间基本就是背单词，练听力。单词我是用了墨墨背单词。听力就是练听写。\n我英语底子超烂，烂到我现在回头看都不敢相信。四六级的听力我从来都没听懂过，一直都是靠猜的。但是我运气很好，最后一次六级因为涂卡涂错了，反倒六级上了500（543）。这个很关键，因为这个成绩刚好出在保研之前，对保研有一定作用。\n刚开始听托福听力，我天，说的啥，完全听不懂。我就练习听写，就是听一句，在电脑上敲一句，听了差不多10多个tpo, 打了50多页的听力文本。这对我的打字速度也很有帮助，大学之前没接触过电脑，打字一直很慢，不会盲打，要看键盘才行。但是托福写作是手打字，而且分数和字数关系很大。\n这一阶段，包括背单词，听写tpo, 和打字练习都算是基本功的练习。\n第二阶段 第一次考试 这时候本科已经结束了，进入研一，这一阶段我的主要精力还是在听力，其次是阅读，在是写作和口语。我花了800块在网上买了二手课，一科200，是ALL IN ONE课程，把上面的视频课都看一了一遍，也报名了2018-11-14的考试，考前就知道肯定过不了，就当第一次熟悉一下流程。\n第一次是在北航考试，离我学校很近，考场环境很好。但确实口语，写作啥都没准备，段子没背，模板没背。考了82，意料之中。这一次是最便宜的一次，1700，之后就涨价了，2000.\n第二次考试 第二次考试本来报名的是2019-2月份左右，但我临时退缩了，因为我觉得肯定过不了，二来得到消息第三年才出国，所以又多了一年准备的时间。就在春节回家后花了600转考了，转到6月1号。\n日子过得很快，研一基本上上课+课题占了一大半时间。这段时间还很浪，具体怎么浪就不细说了。\n这段时间我还是阅读和听力，阅读基本就是去肿瘤的地铁上做一篇，因为我第一次阅读就28了，所以就没花其他时间，听力我没在听写了，只听，精听也不写，就看看听懂没，没听懂再听。听写确实很耗时间，一天啥都不做，也就只能做一个tpo,但如果不写，就快很多。\n但我的口语和写作基本还是没怎么练习。和第一次差多不。所以考前一周，我很抗拒，我请了一周的假，没去医院。当人对某件事很抗拒的时候，他就容易自暴自弃。考前花了四整天，把重装机兵IV 3ds 通关了。\n这次考试还是在北航。我记得考前一天，我和蔡张两位同学去踩点，顺便吃了附近的类似米线的东西。\n本来以为这次没啥进步，但考了90分。我很受鼓舞。觉得提了8分，照这个进度，下次应该能过了。\n第三次考试 这次是我原本预计的最后一次，和前几次一模一样，写作和口语基本还是空白。唯一不同的是这次是在北大计算中心考。我发现我之前真是头铁，明明知道口语和写作提升空间很大，但就是没准备。为什么不准备呢？\n最大的原因就是不知道怎么准备，我唯一的资料就是之前买的800元资料，但那个资料在旧电脑上，新电脑没有，而且我也看过了。用处不大，加上2019-8-1托福改革，新题型我没见过，所以这次说实话希望还是不大。\n北大考场也很不错，体验也会很好。\n成绩出来了，89，比上次还少一分，但best score提了2分。 best score也是这次改革之后才有的。\n没过是我意料之中的。因为我考前就又报了一次2020-1-4的考试。要求2020-2之前出分，所以这确实是最后一次了。\n第四次考试（破釜沉舟） 能报第四次考试也是得益于天时。11月左右学校10000奖学金发下来了。这个很关键，要不我是没钱报托福的。当时确实没钱了，要不我可能9月还会报一次。\n我考托福没找父母要钱，因为太贵了，告诉他们，尤其是我妈，知道了肯定心疼，吃不好睡不好。对于省吃俭用的他们来说花2000考一次试太浪费了，而且还不一定过。所以我奖学金的事情没告诉他们。\n既然经济有了，我决定报班。因为我真的不知道怎么准备。我咨询了很多客服，小站，考满分都问了。他们都好贵。我要性价比高的，我在知乎上看了很多帖子，最后报了28写作黎老师，Fiona（二姐）的口语和听力。这次我确实是动真格了，花了3000+报这些班。\n听力和口语的直播课我都看了，跟班上。但这段时间我的课题和专业课考试方面很紧，超级紧，一次工作汇报，一次文献汇报，还有论文写poster等等。我根本没时间学英语。我很疲惫。考前两周的时候，我三科的作业就交了一次，写作的直播课一次没看。\n12月19号我做完工作汇报，20号照常上班。晚上和室友回到宿舍，我打算彻底放弃了。这时候离考试只剩两周，我26号还有个统计考试。我跟室友说：‘我没希望了！出不去了’. 我真的很庆幸，他们劝我再尝试一把，别去实验室了，脱产复习。\n于是鼓起勇气，认真措辞，跟丽华师姐请了两周假。我也很感谢丽华师姐的理解。\n真正的准备 考前 当然这两周并不是都用来准备托福了。我还花了一天把论文的一些工作收了个尾，花了两天半复习了统计课（没想到平时没咋去上课，考前两周看了ppt竟然考了95.5 A+）。花了半天高中同学聚餐。中间看了几场比赛。 其他的所有时间几乎都是在准备托福。我的主要精力放在写作。我先把所有的直播录播课看完了，按要求把作业都交了，背模板，看两分钟破题，信手拈词精华版。 听力我花了两天把听力四级词汇过了两遍。整个人都想吐。场景词汇和tpo词汇都没来得及过。 口语把模板过了一遍，把15个段子背熟了。 可以说时间太少了，考前我还有好多东西没看完。考前模考了一次，阅读和听力没啥改变，写作能按时间打完。口语还是很垃圾。可以说我其实估计能考个95左右就好。\n考试 这次是在外国语学校，下午考，上午我还在过段子。中午11点就去吃饭了，吃完就去考场了。我去的很早，2：30开考，我12点多就到了。 不得不说，这是我最差的一次考试体验。没有备考区，去了只能站在走廊里，就楼梯的角落里放了两个凳子，我还把头磕到楼梯上了。没有储物柜，东西就放框里，吃的就放窗台上。考场超级挤，正常的一个人的桌子中间放个泡沫隔板，就变成双人桌了。电脑屏幕很小，跟我的pad差不多，键盘就淘宝10块一个的那种，桌子很小，放个键盘就没地写字了。我只能把键盘放到桌底下，记完笔记再把键盘放上来。耳机带着耳朵疼，写作的时候我耳朵都受不了了，只能把耳机脱了写。\n这些这是硬件的，问题是考生也很奇葩，我都阅读开始做好半天了，才进场，就做我左边，紧挨着我，试音超级影响我。还好我前面做的还很顺利，尽管做第一篇的时候很吵，但第一篇很简单，好像是生物类的。我做完前三篇的时候还剩22min,改革之后每篇平均18min，所以我前面做的很快了。但第四篇加试的时候，做了一半的时候，四周想起了听力的声音，为啥做听力要把声音开这么大啊，耳机还漏音，加上第四篇超级难，我直接崩了，后面全瞎选的。\n然后这个人都懵的。听力也没怎么挺懂，口语第一篇正好是背过的段子，问图书馆应不应该保持安静，但我还是没有说完。后面的综合口语也说得很烂，都听懂了，但是表达不出来。\n写作我不得不吐槽。这是我准备最多的，但是我考试体验很不好，还是旁边那位，打字恨不得把键盘凿穿。而且打字我的双肘都没地放，只能悬空。很累，老是打错字。可能是我模板用的很熟，还是打了476个词，我真的很感谢黎老师的模板，很好用。\n考完我就认为凉了。。。\n戏剧反转 真的，我觉得电视剧都没这么演的。 我考完认我绝对过不了。\n差分的过程也很戏剧性。一般六天处分，周五下午，室友就让我查，但我以为我肯定过不了，就查之前很平静。谁知道出的时候，先出的best score, 102 过了，写作和听力爆种了，27。其他两科一个19，一个29，都是之前考的。如果两次一样最高的话，best score只显示最近的，所以说我的阅读最高28，口语最高18，当且仅当这种情况我才能单次成绩上100。我认为这不可能。首先，我阅读崩了，除非最后一篇是加试要不绝对考不到28。而且口语必须18.\n这放在统计里就是概率无限接近0的不可能事件。\n但是晚上8点多的时候。我刚打完一局游戏，查一下，我的天当时我的心情真的跟中了彩票一样，那种不可思议的心情。。。\n总结和展望 当然过了并不代表什么，有很大的运气成分。也有抉择部分，还好我没放弃，还好同学劝了我，还好我请了假，还好师姐和老师给我假，还好考场上我心态崩的时候没有弃考而是及时调整了过来，还好老天让我一分不差的刚好过了。\n过了意味着我要贷款出国，这不是一小笔费用。这又是一个艰难的抉择。既然老天让我刚好过了，就说明这是天意。30万对现在的我来说是天数，但未来总是不知道的，就像我考托福，前面不认真准备，运气再好也过不了，考场在舒适也过不了。只有你努力了，尝试过，才可能加上一些运气成分，取得一些改变。出国一年既是挑战，同样也是机遇。人生来一遭，正当年少，连这点险都不敢冒，不敢做尝试，那活着有什么意义呢？\n最后不得不承认，14是我的幸运数字。我六年级的学号就是14，第一次月考就双百逆袭。。。\n","permalink":"https://xiergo.github.io/posts/life/%E8%80%83%E6%89%98%E7%A6%8F/","summary":"","title":"考托福"},{"content":"ctrl + C 中断当前程序\nctrl+ D 键盘输入结束\ncp\nmv\ntouch\nrm\numask 默认权限\nchmod\nchown 修改文件所属\nnhup \u0026hellip; \u0026amp;\ntop 显示或管理执行中的程序，交互\njobs\nps： ps displays information about a selection of the active processes.\nkill / killall / pkill\nhistory\nwget url 下载\ngzip 压缩\ngunzip 解压\ntar 打包\n文本编辑 wc\n-c 字节数 byte counts\n-l 行数 lines\n-m 字符数 character counts\n-w : words\n1 2 [xieyuhao@ccb-web linuxLearning]$ ls | wc -l 0 less:\n-S\n-N:\nsort\nuniq\ncut\npaste\ntr\nsort\n​\t-r: reverse\n​\t-n: numeric-sort\n​\t-k: sort via a key\n​\t-t: field separator\n​\t-u: output only the first of an equal run (unique)\nuniq\n​\t-c : prefix lines by the number of occurrences\n​\t-d: only print duplicate lines, one for each group\n​\t-u: only print uniq lines\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 [xieyuhao@ccb-web week2]$ cat demo1.txt aaa:10:1.1 ccc:30:3.3 ddd:40:4.4 bbb:20:2.2 eee:50:5.5 [xieyuhao@ccb-web week2]$ sort -nk2 -t : demo1.txt aaa:10:1.1 bbb:20:2.2 ccc:30:3.3 ddd:40:4.4 eee:50:5.5 [xieyuhao@ccb-web week2]$ cat demo2.txt aaaaaaaaaaaaa bbbbbb bbbbbb cc cc cc [xieyuhao@ccb-web week2]$ sort -u demo2.txt aaaaaaaaaaaaa bbbbbb cc [xieyuhao@ccb-web week2]$ uniq demo2.txt aaaaaaaaaaaaa bbbbbb cc [xieyuhao@ccb-web week2]$ uniq -c demo2.txt 1 aaaaaaaaaaaaa 2 bbbbbb 3 cc [xieyuhao@ccb-web week2]$ uniq -d demo2.txt bbbbbb cc [xieyuhao@ccb-web week2]$ cat seasons winter winter spring spring spring summer summer summer fall fall fall winter [xieyuhao@ccb-web week2]$ uniq seasons winter spring summer fall winter [xieyuhao@ccb-web week2]$ sort seasons|uniq fall spring summer winter [xieyuhao@ccb-web week2]$ sort -u seasons fall spring summer winter [xieyuhao@ccb-web week2]$ uniq -c seasons 2 winter 3 spring 3 summer 3 fall 1 winter cut\n​\t-d: field delimiter=DELIM. (default TAB)\n​\t-f : fields=LIST , select these fields; also print any line that contains no delimiter character, unless the -s option is specified\n​\t-s: not print lines without delimiters\n​\t-c: complements the set of selected bytes, characters or fields\n​\t-b: \u0026ndash;bytes=LIST, select only these bytes\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 [xieyuhao@ccb-web week2]$ cat demo3.txt stu No\tName\tMark\tPercent 01\ttom\t69\t91 02\tjack\t71\t87 03\talex\t68\t98 [xieyuhao@ccb-web week2]$ cut -f1,3 demo3.txt stu No\tMark 01\t69 02\t71 03\t68 [xieyuhao@ccb-web week2]$ cut -c1,2,3 demo3.txt stu 01\t02\t03\t[xieyuhao@ccb-web week2]$ cut -b1,2,3 demo3.txt stu 01\t02\t03\t[xieyuhao@ccb-web week2]$ cut -b1-3 demo3.txt stu 01\t02\t03\t[xieyuhao@ccb-web week2]$ cut -b1-5 demo3.txt stu N 01\tto 02\tja 03\tal [xieyuhao@ccb-web week2]$ cut -c -6 demo3.txt stu No 01\ttom 02\tjac 03\tale paste 多个文件按列合并\n​\t-d： \u0026ndash;delimiters=LIST, reuse characters from LIST instead of TABs\ntr : 将字符进行替换压缩和删除(translate)\n​\ttr (options) SET1 [SET2]\n​\t-d: delete\n​\t原字符集 $\\to$ 新字符集\n​\t没有SET2，必须指定-d 就是delete\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 [xieyuhao@ccb-web week2]$ cat demo1.txt demo2.txt aaa:10:1.1 ccc:30:3.3 ddd:40:4.4 bbb:20:2.2 eee:50:5.5 aaaaaaaaaaaaa bbbbbb bbbbbb cc cc cc [xieyuhao@ccb-web week2]$ paste demo2.txt demo1.txt demo1.txt demo2.txt |awk -F \u0026#39;\\t\u0026#39; -v OFS=\u0026#39;,\u0026#39; -v SQ=\u0026#34;\u0026#39;\u0026#34; \u0026#39;{for (i=1;i\u0026lt;=NF;i++) {printf \u0026#34;%s%s%s%s\u0026#34;,SQ,$i,SQ,(i\u0026lt;NF)?OFS:ORS}}\u0026#39; \u0026#39;aaaaaaaaaaaaa\u0026#39;,\u0026#39;aaa:10:1.1\u0026#39;,\u0026#39;aaa:10:1.1\u0026#39;,\u0026#39;aaaaaaaaaaaaa\u0026#39; \u0026#39;bbbbbb\u0026#39;,\u0026#39;ccc:30:3.3\u0026#39;,\u0026#39;ccc:30:3.3\u0026#39;,\u0026#39;bbbbbb\u0026#39; \u0026#39;bbbbbb\u0026#39;,\u0026#39;ddd:40:4.4\u0026#39;,\u0026#39;ddd:40:4.4\u0026#39;,\u0026#39;bbbbbb\u0026#39; \u0026#39;cc\u0026#39;,\u0026#39;bbb:20:2.2\u0026#39;,\u0026#39;bbb:20:2.2\u0026#39;,\u0026#39;cc\u0026#39; \u0026#39;cc\u0026#39;,\u0026#39;eee:50:5.5\u0026#39;,\u0026#39;eee:50:5.5\u0026#39;,\u0026#39;cc\u0026#39; \u0026#39;cc\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;\u0026#39;,\u0026#39;cc\u0026#39; [xieyuhao@ccb-web week2]$ paste -d \u0026#39;hi\u0026#39; demo2.txt demo1.txt demo1.txt demo2.txt aaaaaaaaaaaaahaaa:10:1.1iaaa:10:1.1haaaaaaaaaaaaa bbbbbbhccc:30:3.3iccc:30:3.3hbbbbbb bbbbbbhddd:40:4.4iddd:40:4.4hbbbbbb cchbbb:20:2.2ibbb:20:2.2hcc ccheee:50:5.5ieee:50:5.5hcc cchihcc [xieyuhao@ccb-web week2]$ echo Test| tr \u0026#39;Test\u0026#39; \u0026#39;ab\u0026#39; abbb [xieyuhao@ccb-web week2]$ echo Test| tr \u0026#39;Test\u0026#39; \u0026#39;abcde\u0026#39; abcd [xieyuhao@ccb-web week2]$ echo Test| tr \u0026#39;A-Z\u0026#39; \u0026#39;a-z\u0026#39; test regular expression 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 + ? | () ()+ . \\w \\s \\d \\W \\S \\D [] [^...] 限定符 * + ? {n} {n,} {,m} {n,m} ^ $ \\b \\\u0026lt; 匹配单词开始 \\\u0026gt; 匹配单词结尾 \\(\\)中的内容可以用\\1引用 [xieyuhao@ccb-web week2]$ echo hihihi|grep \u0026#39;\\(hi\\)\\1\\1\u0026#39; hihihi [xieyuhao@ccb-web week2]$ echo hihihi|grep \u0026#39;\\(hi\\){1,3}\u0026#39; [xieyuhao@ccb-web week2]$ echo hihihi|grep \u0026#39;\\(hi\\)\\{1,3\\}\u0026#39; hihihi linux 三剑客 grep\ngrep [options] pattern [file]\n-E extended\n-c only print a count of matching lines for each input files\n-f obtain patterns from file, one per line\n-i ignore case\n-l 打印出匹配上的输入文件名\n-A/B 多输出after/before行（前后）\n-? ?为数值, 多出附近？行\n-n prefix each line with line number within its input file\n-v invert match(没匹配上的)\n-x 精确匹配 （匹配上一整行）\nsed\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 命令 s: 替换 N: 会把原文本中的偶数行纳入奇数行匹配 a: append i: insert c: 替换匹配行 d:删除匹配行 y: 对应字符替换 p: 打印 =： 打印行号 -i ： 直接在文件上修改 -n: 抑制自动打印，和p零一o\u0026#39;n\u0026#39;g s/^/#/g s/$/ ----/g 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 [xieyuhao@ccb-web week2]$ sed -n \u0026#39;/number 2/=\u0026#39; test6.txt 2 $ sed \u0026#39;s/s/S/3g\u0026#39; my.txt 三个以后的匹配 $ sed \u0026#39;1,3s/my/your/g; 3,$s/This/That/g\u0026#39; my.txt sed -e \u0026#39;1,3s/my/your/g\u0026#39; -e \u0026#39;3,$s/This/That/g\u0026#39; my.txt#tong shang #我们可以使用\u0026amp;来当做被匹配的变量，然后可以在基本左右加点东西 $ sed \u0026#39;s/my/[\u0026amp;]/g\u0026#39; my.txt $ sed \u0026#39;N;s/my/your/\u0026#39; pets.txt # 其中的1i表明，其要在第1行前插入一行（insert） $ sed \u0026#34;1 i This is my monkey, my monkey\u0026#39;s name is wukong\u0026#34; my.txt # 其中的1a表明，其要在最后一行后追加一行（append） $ sed \u0026#34;1 a This is my monkey, my monkey\u0026#39;s name is wukong\u0026#34; my.txt # 注意其中的/fish/a，这意思是匹配到/fish/后就追加一行 $ sed \u0026#34;/fish/a This is my monkey, my monkey\u0026#39;s name is wukong\u0026#34; my.txt $ sed \u0026#34;/fish/c This is my monkey, my monkey\u0026#39;s name is wukong\u0026#34; my.txt $ sed \u0026#39;/fish/d\u0026#39; my.txt $ sed \u0026#39;2,$d\u0026#39; my.txt #删除2到文档结尾 $ sed -n \u0026#39;/fish/p\u0026#39; my.txt This is my fish, my fish\u0026#39;s name is george # 从一个模式到另一个模式 $ sed -n \u0026#39;/dog/,/fish/p\u0026#39; my.txt This is my dog, my dog\u0026#39;s name is frank This is my fish, my fish\u0026#39;s name is george #从第一行打印到匹配fish成功的那一行 $ sed -n \u0026#39;1,/fish/p\u0026#39; my.txt This is my cat, my cat\u0026#39;s name is betty This is my dog, my dog\u0026#39;s name is frank This is my fish, my fish\u0026#39;s name is george pattern space\n1 2 3 4 5 6 7 8 9 10 11 12 foreach line in file { //放入把行Pattern_Space Pattern_Space \u0026lt;= line; // 对每个pattern space执行sed命令 Pattern_Space \u0026lt;= EXEC(sed_cmd, Pattern_Space); // 如果没有指定 -n 则输出处理后的Pattern_Space if (sed option hasn\u0026#39;t \u0026#34;-n\u0026#34;) { print Pattern_Space } } 地址（address）\n[address[,address]][!]{cmd}\n1 2 3 x x,y x~y: x行开始，每隔y行 !不执行\naddress可以是一个数字，也可以是一个模式，你可以通过逗号要分隔两个address 表示两个address的区间，参执行命令cmd，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 bool bexec = false foreach line in file { if ( match(address1) ){ bexec = true; } if ( bexec == true) { EXEC(sed_cmd); } if ( match (address2) ) { bexec = false; } } 1 2 3 # address可以使用相对位置，其中的+3表示后面连续3行 $ sed \u0026#39;/dog/,+3s/^/# /g\u0026#39; pets.txt 命令打包\n1 2 3 4 5 #第二个是cmd可以是多个，它们可以用分号分开，可以用大括号括起来作为嵌套命令 # 对3行到第6行，匹配/This/成功后，再匹配/fish/，成功后执行d命令 $ sed \u0026#39;3,6 {/This/{/fish/d}}\u0026#39; pets.txt # 从第一行到最后一行，如果匹配到This，则删除之；如果前面有空格，则去除空格 $ sed \u0026#39;1,${/This/d;s/^ *//g}\u0026#39; pets.txt hold space g： 将hold space中的内容拷贝到pattern space中，原来pattern space里的内容清除 G： 将hold space中的内容append到pattern space\\n后 h： 将pattern space中的内容拷贝到hold space中，原来的hold space里的内容被清除 H： 将pattern space中的内容append到hold space\\n后 x： 交换pattern space和hold space的内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ cat t.txt one two three $ sed \u0026#39;H;g\u0026#39; t.txt one one two one two three #反序了一个文件的行 $ sed \u0026#39;1!G;h;$!d\u0026#39; t.txt three two one awk\nawk [options] awk_script input_file -F fs 指定行中分割字段的字段分隔符 -f 指定读取程序的文件名 -v var=value 定义一个变量及其默认值\n1 2 3 4 5 6 7 8 #没pattern默认匹配， 没action默认打印行 awk \u0026#39;BEGIN {action}\\ pattern1 {}\\ pat2{}\\ .. patn{}\\ END {}\u0026#39;\\ input_files awkpattern: ==,\u0026gt; , ~/regexp/, \u0026amp;\u0026amp; ||\nawk 命令 print printf next nextfile exit: 跳出循环，不跳过end\n1 2 3 4 5 6 7 # 计算当目录的文件总大小 [xieyuhao@ccb-web week2]$ ll|awk \u0026#39;BEGIN {sum=0} {sum+=$5} END {print sum}\u0026#39; 10345281 # test.gtf中 chr1: 12000~15000的外显子 [xieyuhao@ccb-web week2]$ cat test.gtf | awk \u0026#39;(($1~/chr1/)\u0026amp;\u0026amp;($3~/exon/) \u0026amp;\u0026amp; ($4\u0026gt;12000)\u0026amp;\u0026amp;($5\u0026lt;15000))||/^#/ \u0026#39;|wc -l 23 vim x: delete the character under the cursor\ni 光标前插入\nA 行尾append\na: 光标后append\ndw: 从光标处删到下一个单词开头\nd$: 从光标删到行尾\nde: 从光标删到当前词尾\noperator+ motions\nmotion: $ e w ^(当前行的第一个非空字符)\n只用motion，只移动光标不操作\nnumber+ motion\ndd: delete a whole line\nd2d=2dd: delete two lines\nu: undo the last commands\nU: fix a whole line\nctrl+r : undo the undo\u0026rsquo;s\np: to put previously deleted text after the cursor\nr+x: 将光标处的替换为x\nR: 替换多个字符\nc: change operator\nctrl + g: show the location in the file and the file status.\nG : move to the bottom of the file\ngg: move to the start of the file\nnum+ G : move to the num line\n/search (? 反向搜)\nn: 下一个\nN: 上一个\nctrl+ o : 回到上一次的光标位置\nctrl + i : 和上面相反756G\n%： find a matching )]}\ns/old/new/g : substitute (global)\n#,#s/old/new/g , where #,# are the line numbers of the range\n%s/old/new/g: 替换整个file\n%s///gc 同上，但每一处都询问一下\n:!command 执行外部命令\n:w filename 另存为filename\nv motion : 选中（visual mode）\n:r filename 在光标下面插入文本\n:r !ls同上\no: 光标下一行插入模式\nO: 光标上一行插入\ny: copy (yank)\np: paste\nset ic : ignore case\nset noic\n:help\ndf: 整体磁盘使用量\n​\t-a : all\n​\t-h : human\n​\t-T\n磁盘阵列： raid\n​\tredundant arrays of inexpensive disks\n​\traid-0: 性能佳\n​\traid-1:最安全\n​\traid-5: 中间\n后台运行 1 2 3 nohup \u0026amp; top ncbi\nembl\nucsc\n解压： gzip/gunzip\n打包: tar\n10-10 变量定义\n没空格\n英文数字下划线，数字不开头\n若变量要在其他子进程执行，要export使变量变成全局变量\n只读变量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 (base) [student-1811210054@localhost week3]$ aa=123 (base) [student-1811210054@localhost week3]$ readonly aa (base) [student-1811210054@localhost week3]$ aa=55 -bash: aa: readonly variable (base) [student-1811210054@localhost week3]$ aa bash: aa: command not found... (base) [student-1811210054@localhost week3]$ echo aa aa (base) [student-1811210054@localhost week3]$ echo $aa 123 (base) [student-1811210054@localhost week3]$ unset aa -bash: unset: aa: cannot unset: readonly variable (base) [student-1811210054@localhost week3]$ (base) [student-1811210054@localhost week3]$ (base) [student-1811210054@localhost week3]$ a=1+2+3 (base) [student-1811210054@localhost week3]$ echo $a 1+2+3 (base) [student-1811210054@localhost week3]$ declare -i a=1+3+3 (base) [student-1811210054@localhost week3]$ echo $a 7 (base) [student-1811210054@localhost week3]$ a=terera (base) [student-1811210054@localhost week3]$ echo $a 0 数组 0-based\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 (base) [student-1811210054@localhost week3]$ v[0]=1 (base) [student-1811210054@localhost week3]$ v[1]=44 (base) [student-1811210054@localhost week3]$ v[2]=4423 (base) [student-1811210054@localhost week3]$ echo $v 1 (base) [student-1811210054@localhost week3]$ echo $v[2] 1[2] (base) [student-1811210054@localhost week3]$ echo ${v[2]} 4423 (base) [student-1811210054@localhost week3]$ ehco ${v[@]} bash: ehco: command not found... Similar command is: \u0026#39;echo\u0026#39; (base) [student-1811210054@localhost week3]$ echo ${v[@]} 1 44 4423 (base) [student-1811210054@localhost week3]$ echo ${v[*]} 1 44 4423 (base) [student-1811210054@localhost week3]$ echo ${#v} 1 (base) [student-1811210054@localhost week3]$ echo ${#v[*]} 3 (base) [student-1811210054@localhost week3]$ a=(1 32 aaa) (base) [student-1811210054@localhost week3]$ echo ${a[@]} 1 32 0 (base) [student-1811210054@localhost week3]$ aaa bash: aaa: command not found... (base) [student-1811210054@localhost week3]$ echo $aaa (base) [student-1811210054@localhost week3]$ b=(1 fa fdafa) (base) [student-1811210054@localhost week3]$ echo ${b[@]} 1 fa fdafa 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ##. source 在父进程进行,不开新进程 ##./runtest 开子进程 [xieyuhao@ccb-web week3]$ cat runTest.sh unset var var=\u0026#39;test\u0026#39; echo \u0026#34;the value of var is $var\u0026#34; [xieyuhao@ccb-web week3]$ ./runTest.sh the value of var is test [xieyuhao@ccb-web week3]$ echo $var [xieyuhao@ccb-web week3]$ source runTest.sh the value of var is test [xieyuhao@ccb-web week3]$ echo $var test [xieyuhao@ccb-web week3]$ unset var [xieyuhao@ccb-web week3]$ echo $var [xieyuhao@ccb-web week3]$ . runTest.sh the value of var is test [xieyuhao@ccb-web week3]$ echo $var test （（）） 转义（）\n1 2 3 (base) [student-1811210054@localhost week3]$ x=12 (base) [student-1811210054@localhost week3]$ y=`echo \u0026#34;$x 5\u0026#34;|awk \u0026#39;{printf($1+$2)}\u0026#39;` ;echo $y 17 循环 for循环 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 for var in list do 命令 done for i in $(seq 1 10) for i in {1..10} for ((i=1;i\u0026lt;11;i++)) for i in `ls` root@localhost ~]# for ((i=0;i\u0026lt;5;i++));do echo n $i;done [root@localhost ~]# for i in $(seq 0 4);do echo n $i;done [root@localhost ~]# for i in `seq 0 4`;do echo n $i;done for place in Alabama Alaska Arizona Arkansas California Colorado do echo \u0026#34;Have you ever visited ${place}?\u0026#34; done # special character in list for test in I don\\\u0026#39;t know if \u0026#34;this\u0026#39;ll\u0026#34; work do echo \u0026#34;word:$test\u0026#34; done # variable as list list=\u0026#34;Alabama Alaska Arizona Arkansas Colorado\u0026#34; list=$list\u0026#34; Connecticut\u0026#34; for state in $list do echo \u0026#34;Have you ever visited ${state}?\u0026#34; done # command as list for file in `ls` do echo \u0026#34;file: $file\u0026#34; done Have you ever visited Alabama? Have you ever visited Alaska? Have you ever visited Arizona? Have you ever visited Arkansas? Have you ever visited California? Have you ever visited Colorado? word:I word:don\u0026#39;t word:know word:if word:this\u0026#39;ll word:work Have you ever visited Alabama? Have you ever visited Alaska? Have you ever visited Arizona? Have you ever visited Arkansas? Have you ever visited Colorado? Have you ever visited Connecticut? file: art.lambda.cov1001.fq file: art.lambda.cov1002.fq file: breakTest.sh file: caseTest.sh file: continueTest.sh file: empty.txt file: expr.calc file: forTest.sh file: getoptsTest2.sh file: getoptsTest.sh file: getoptTest.sh file: ifTest.sh file: lambda.sample1.R1.fastq 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/bin/bash #ifTest #to show the method of if read -p \u0026#34;Enter the first integer: \u0026#34; FIRST read -p \u0026#34;Enter the second integer: \u0026#34; SECOND if [ \u0026#34;$FIRST\u0026#34; -gt \u0026#34;$SECOND\u0026#34; ] then echo \u0026#34;$FIRST is greater than $SECOND\u0026#34; elif [ \u0026#34;$FIRST\u0026#34; -lt \u0026#34;$SECOND\u0026#34; ] then echo \u0026#34;$FIRST is less than $SECOND\u0026#34; else echo \u0026#34;$FIRST is equal to $SECOND\u0026#34; fi 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #!/bin/bash var=5 while [ $var -gt 0 ] do echo $var var=$[ $var - 1 ] done [xieyuhao@ccb-web week3]$ var=4;while (($var\u0026gt;0));do echo $var ; var=$var-1;done 4 4-1 4-1-1 4-1-1-1 [xieyuhao@ccb-web week3]$ var=4;while (($var\u0026gt;0));do echo $var ; var=$(($var-1));done 4 3 2 1 [xieyuhao@ccb-web week3]$ var=4;while (($var\u0026gt;0));do echo $var ; var=$[$var-1];done 4 3 2 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 # breaking out of a for 1oop for var in 1 2 3 4 5 6 7 8 9 10 do if [ $var -eq 5 ] then break fi echo \u0026#34;Iteration number:$var\u0026#34; done echo \u0026#34;The for loop is completed\u0026#34; # breaking out of an inner loop echo \u0026#34;--------------------------------\u0026#34; for ((a=1;a\u0026lt;4;a++)) do echo \u0026#34;Outer loop:$a\u0026#34; for(( b = 1; b \u0026lt; 100; b++ )) do if [ $b -eq 5 ] then break fi echo \u0026#34;Inner loop:$b\u0026#34; done done # breaking out of an outer 1oop echo \u0026#34;---------------------------------\u0026#34; for (( a=1;a\u0026lt;4;a++)) do echo \u0026#34;Outer loop:$a\u0026#34; for((b=1;b\u0026lt;100;b++)) do if [ $b -gt 4 ] then break 2 fi echo \u0026#34;Inner loop:$b\u0026#34; done done Iteration number:1 Iteration number:2 Iteration number:3 Iteration number:4 The for loop is completed -------------------------------- Outer loop:1 Inner loop:1 Inner loop:2 Inner loop:3 Inner loop:4 Outer loop:2 Inner loop:1 Inner loop:2 Inner loop:3 Inner loop:4 Outer loop:3 Inner loop:1 Inner loop:2 Inner loop:3 Inner loop:4 --------------------------------- Outer loop:1 Inner loop:1 Inner loop:2 Inner loop:3 Inner loop:4 1 2 3 4 5 6 7 8 9 10 11 12 13 14 [xieyuhao@ccb-web week3]$ echo \u0026#39;$a\u0026#39; $a [xieyuhao@ccb-web week3]$ echo \u0026#34;$a\u0026#34; 1 [xieyuhao@ccb-web week3]$ a=\u0026#34;wo shi jj\u0026#34; [xieyuhao@ccb-web week3]$ echo $a wo shi jj [xieyuhao@ccb-web week3]$ echo ${#a}# length 9 [xieyuhao@ccb-web week3]$ echo ${a:1:4} o sh [xieyuhao@ccb-web week3]$ echo ${a:1:100} o shi jj ${str:start:length} # 0-based 运算符\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 -d file -f file -s file # 文件存在且不为空，返回true -e file # 检测文件（包括目录）是否存在 -r file -w file -x file command1 \u0026amp;\u0026amp; command2 || command3 if(command1); then command2; else command3;fi [xieyuhao@ccb-web week3]$ test -d while.sh \u0026amp;\u0026amp; echo dir || echo not dir not dir ## test expr 和[ expr ]是等价的(必须有空格) [xieyuhao@ccb-web week3]$ [ -d while.sh ]\u0026amp;\u0026amp; echo dir||echo not dir not dir [xieyuhao@ccb-web week3]$ [[ -d while.sh ]]\u0026amp;\u0026amp; echo dir||echo not dir not dir [xieyuhao@ccb-web week3]$ [[ \u0026#39;a\u0026#39; -a \u0026#39;b\u0026#39; ]]\u0026amp;\u0026amp; echo dir||echo not dir -bash: conditional binary operator expected -bash: syntax error near `-a\u0026#39; [xieyuhao@ccb-web week3]$ [[ \u0026#39;a\u0026#39; \u0026amp;\u0026amp; \u0026#39;b\u0026#39; ]]\u0026amp;\u0026amp; echo dir||echo not dir dir [xieyuhao@ccb-web week3]$ [ \u0026#39;a\u0026#39; -a \u0026#39;b\u0026#39; ] \u0026amp;\u0026amp; echo dir||echo not dir dir 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # 运算符 + - \\* / 取商（整数） % 取余 = == ！= # 乘号要转义 [xieyuhao@ccb-web week3]$ expr 10 * 10 expr: syntax error [xieyuhao@ccb-web week3]$ expr 10 \\* 10 100 #注意空格 [xieyuhao@ccb-web week3]$ expr 1 + 1 2 [xieyuhao@ccb-web week3]$ expr 1 +1 expr: syntax error [xieyuhao@ccb-web week3]$ expr 1+ 1 expr: syntax error [xieyuhao@ccb-web week3]$ expr 1+1 1+1 # $[] $(())不用注意空格 [xieyuhao@ccb-web week3]$ x=12 [xieyuhao@ccb-web week3]$ y=$[x+7] [xieyuhao@ccb-web week3]$ z=$((x+8)) [xieyuhao@ccb-web week3]$ z=$((x+8)) [xieyuhao@ccb-web week3]$ echo $y $z 19 20 [xieyuhao@ccb-web week3]$ y=$[x-7] [xieyuhao@ccb-web week3]$ z=$(( x - 8 )) [xieyuhao@ccb-web week3]$ echo $y gg $z 5 gg 4 # declare -i 定义一个整数 # declare -r read-only case 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 [xieyuhao@ccb-web week3]$ cat caseTest.sh #!/bin/sh option=\u0026#34;${1}\u0026#34; case ${option} in -f) FILE=\u0026#34;${2}\u0026#34; echo \u0026#34;File name is $FILE\u0026#34; ;; -d) DIR=\u0026#34;${2}\u0026#34; echo \u0026#34;Dir name is $DIR\u0026#34; ;; *) # 如果参数非\u0026#34;-f\u0026#34;或\u0026#34;-d\u0026#34; echo \u0026#34;`basename ${0}`:usage: [-f file] | [-d directory]\u0026#34; exit 1 # Command to come out of the program with status 1 ;; esac [xieyuhao@ccb-web week3]$ sh caseTest.sh -d ~/.bashrc Dir name is /home/xieyuhao/.bashrc [xieyuhao@ccb-web week3]$ sh caseTest.sh -hh ~/.bashrc caseTest.sh:usage: [-f file] | [-d directory] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 #参数传递 #!/bin/bash #varTest.sh #To test the variables USER=username echo \u0026#34;Hello, ${USER}, the output of this script are as follows:\u0026#34; echo \u0026#34;The script name is:\t`basename $0`\u0026#34; echo \u0026#34;The first param of the script is:\t$1\u0026#34; echo \u0026#34;The second param of the script is:\t$2\u0026#34; echo \u0026#34;The tenth param of the script is:\t$10\u0026#34; echo \u0026#34;All the params you input are:\t$@\u0026#34; echo \u0026#34;The number of the params you input is:\t$#\u0026#34; echo \u0026#34;The process ID for this script is:\t$$\u0026#34; echo \u0026#34;The exit status of last command:\t$?\u0026#34; tree echo \u0026#34;The exit status of command tree:\t$?\u0026#34; [xieyuhao@ccb-web week3]$ sh varTest.sh p{1..12} Hello, username, the output of this script are as follows: The script name is:\tvarTest.sh The first param of the script is:\tp1 The second param of the script is:\tp2 The tenth param of the script is:\tp10 All the params you input are:\tp1 p2 p3 p4 p5 p6 p7 p8 p9 p10 p11 p12 The number of the params you input is:\t12 The process ID for this script is:\t56737 The exit status of last command:\t0 . |-- art.lambda.cov1001.fq |-- art.lambda.cov1002.fq |-- breakTest.sh |-- case1.sh |-- caseTest.sh |-- continueTest.sh |-- empty.txt |-- expr.calc |-- forTest.sh |-- getopt.sh |-- getoptsTest2.sh |-- getoptsTest.sh |-- getoptTest.sh |-- ifTest.sh |-- lambda.sample1.R1.fastq |-- lambda.sample1.R2.fastq |-- lambda.sample2.R1.fastq |-- lambda.sample2.R2.fastq |-- lambda.sample3.R1.fastq |-- lambda.sample3.R2.fastq |-- lambda.sample4.R1.fastq |-- lambda.sample4.R2.fastq |-- lambda.sample5.R1.fastq |-- lambda.sample5.R2.fastq |-- lambda.sample6.R1.fastq |-- lambda.sample6.R2.fastq |-- lambda.sample7.R1.fastq |-- lambda.sample7.R2.fastq |-- lambda.sample8.R1.fastq |-- lambda.sample8.R2.fastq |-- operator1.sh |-- operator2.sh |-- operator3.sh |-- operator4.sh |-- plot2.sh |-- plot.sh |-- runTest.sh |-- script.sh |-- varTest.sh `-- while.sh 0 directories, 40 files The exit status of command tree: 将测序数据比对到参考基因组上\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 workingdir=~/lambda/ outdir=~/lambda/bwa_output bwa=/home/student/miniconda3/bin/bwa samtools=/home/student/miniconda3/bin/samtools bwa_index=/home/student/data/ref_lambda/index_bwa/lambda for i in `ls lambda.sample*.R1.fastq`;do sample=`echo $i| sed \u0026#39;s/.R1.fastq//\u0026#39;` output=$outdir.${sample}.sorted.bam $bwa mem \\ ${bwa_index}\\ $workdir/${sample}.R1.fastq \\ $workdir/${sample}.R2.fastq | $samtools sort -o $outdir/${sample}.sorted.bam \u0026amp;\u0026amp; \\ if [ -s $output ]; then echo \u0026#34;${sample} mapping finished\u0026#34; fi done 略\n20 填空\n20 判断\n2 * 30 编程\n考试地点： 802\n判断 test [expression]\n返回测试结果的exit status\n表达式为真，返回0， 否则返回非0的假值\n\u0026amp;\u0026amp; 与\n|| 或\n","permalink":"https://xiergo.github.io/posts/tech/%E7%A0%94%E4%BA%8C%E4%B8%8A%E8%AF%BE%E7%A8%8B%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%E6%80%BB%E7%BB%93-linux/","summary":"","title":"研二上课程期末复习总结 Linux"},{"content":"新托福（2019-8-1之后的考试）\n组成部分 4题 ： 一道独立口语；三道综合口语\n考试时间17min\nIn question 1, you will speak about a familiar topic. Your response will be scored on your ability to speak clearly and coherently about the topic.\nprepare: 15 s\nspeaking: 45 s\nIn questions 2 and 3. You will first read a short text. The text will go away and you will then listen to a talk on the same topic. You will then be asked a question about what you have read and heard. You will need to combine appropriate information from the text and the talk to provide a complete answer to the question. Your response will be scored on your ability to speak clearly and coherently and on your ability to accurately convey information about what you have read and heard.\nIn question 4, you will listen to part of a lecture. You will then be asked a question about what you have heard. Your response will be scored on your ability to speak clearly and coherently and on your ability to accurately convey information about what you heard.\n独立口语部分 近期考题速览 2019-10-12\nIf you are going to travel to a new country for a week, do you prefer to travel to a major city in the country or to several different places?\n2019-9-22（同2018-12-16的 task2）\nSome people prefer to stay in touch with their friends and family members while traveling. Others prefer not to get in touch and stay alone. Which do you prefer and why?\n2019-9-21（同2018-10-20的 task2）\nSince students now all have laptops, the university decides to close the computer lab. Do you think this is a good idea? Why or Why not? Use examples and details in your answer.\n简介 15秒准备; 45秒答题\n万能思路 1. Being open minded 语言表达:\n1) open up mind 开阔视野\npeople and cultures 接触到不同的人和文化\nembrace differences. eg: values, beliefs, ideas, and lifestyles\nbecome tolerant 变得更包容\nfind a kind of life $\\rightarrow$ feel comfortable with\n2) enrich experience 丰富经历\nsee and contact with something different and new $\\leftarrow$ one\u0026rsquo;s own society\nshare what (I) have gone through with others\nbecome outgoing and sociable\nmake more friends\nExample 1: your friend has been offered a job that\u0026rsquo;s far away from his hometown. Do you think he should take it or not? Give specific reasons and details to support your response.\nPersonally speaking, I will suggest my friend take this job. because working far away from hometown means visiting different places, which will be beneficial to him for the following reasons.\nFirst, living far away from hometown can help open up his mind, because it will give him a chance to get in touch with different people and cultures, therefore to embrace many differences, such as different values, beliefs, ideas and lifestyles. This experience can not only make him become tolerant to difference, but also allow him to discover and live a life which he feels comfortable with.\nAlso, visiting different places can enrich his experience, because he will see and contact with something different and new from his own society, making him share what he has gone through with others. Such experience will allow him to become more outgoing and sociable and enable him to make more friends.\nExample 2: Do you prefer to travel on business frequently or work on the same location?\nPersonally speaking, I prefer to travel on business frequently instead of working on the same location, because visiting different places can be very beneficial to me. And I have the following reasons.\nFirst, visiting different places on business trip can open up my mind, because I will have a chance to get in touch with different people and cultures and therefore, to embrace many differences such as different values, beliefs, ideas and lifestyles. This experience can not only make me become tolerant to difference, but also allow me to discover and live a life I feel comfortable with.\nAlso, taking business trips can enrich my experience, because I will see and contact with something new and different from my own society, making me share what I have gone through with others. Such experience will allow me to become outgoing and sociable, and enable me to make more friends.\nExample 3: Talk about a thing you always wanted to do but didn\u0026rsquo;t have time to. Explain why you want to do it.\nPersonally speaking, the thing I always want to do is to travel to foreign countries, because I believe visiting different places can benefit my life. And I have the following reasons.\nFirst, traveling can open up my mind, because I will have a chance to get in touch with different people and cultures, therefore to embrace many differences, such as different values, beliefs, ideas and lifestyles. This experience can not only make me become tolerant to difference, but also allow me to discover and live a life I feel comfortable with.\nAlso, taking a trip to foreign counties can enrich my experience, because I will see and contact with something new and different from my own society, making me share what I have gone through with others. Such experience will allow me to become more outgoing and sociable, and enable me to make more friends.\n","permalink":"https://xiergo.github.io/posts/tech/toefl-speaking/","summary":"","title":"Toefl Speaking"},{"content":"序 这是我第一次用博客写电影观后感，其实更像是小学生的日记（又名，流水账）。\n今天，在医院完成一天的研究僧的科研 (搬砖)任务，突然想去看个电影放松一下，又是单身狗一枚，只能一个人去了。再者庆祝一下，明天就放假了，国庆小长假走起 。。。\n那看什么呢，可供选择的有三部口碑不错的国庆档：\n我和我的祖国 中国机长 攀登者 三部我都想看，但看了看卡里可怜的余额，我放弃了这个念头，那就我和我的祖国了，毕竟国庆，接受一下爱国教育也挺好。打开陌生的app淘票票（平时都是舍友买票），一看，今晚的全是满座，就有一个杜比的还有第一排几个空座。。。 看来大家都很爱国啊。犹豫再三，一咬牙还是买了，80大洋没了。时间也挺好，5点45，离医院也很近。吃过饭我就去了。\n电影院全是人，好多来看这部片的，进场后，满怀期待。之前在医院干活的时候听后面的大妈议论这个片的时候，已经了解到这部片是由多个故事串起来的，她们说拍的还不错。影院很大，杜比音效也很不错，就是第一排外加屏幕太大，可能有点不太适应。我旁边是一个幼儿园的小妹妹，全程问妈妈问题，要不是这小姑娘长得太可爱了，我肯定要在心里诅咒她了。\n简要回顾 影片的开头是王菲唱的我和我的祖国，唱的很好听，接下来是一个个故事：\n1. 前夜 导演：管虎\n演员：黄渤，佟大为\n讲的是新中国成立成立前一天晚上，黄渤带领一帮人解决电动升旗装置的事。影片比较感人的地方是大家伙捐东西的场景： 因为固定旗子的一个金属球不符合标准坏掉了，要重新铸一个，然而缺少几种金属，可以从收音机等东西里提取，然后大家都过来捐东西，大家拿着自己很珍贵的东西排着老长的队伍，有拿传家宝的，有拿金条的，电灯的。。。甚至有人把勺子都拿来了。很明显大家在饱受战争带给他们的苦难后，都太想有一个安定的家，有一个能够保护他们的国，没有什么比这个更珍贵了，为了这个什么都能往外捐。老百姓的愿望是很朴实的。黄渤恐高爬旗杆也还行，演得不错，表情很到位。\n2. 相遇 导演： 张一白\n演员： 张泽，任素汐，周冬雨\n这一段是关于中国第一颗原子弹的成功爆炸的故事。故事一开始，为了解决一次研发过程中的危机，张泽受到核辐射，在医院里他看报纸，问护士，问医生，最近有没有什么大事发生，那种期待听到什么消息，得知没有啥事后的那种失落，都表现得很好。出去坐公交车的那一段，开始很不理解，为啥能偶遇女主，而且女主认出带了口罩的他，追着给讲曾经他们之间发生的一些事情，看起来很突兀。后来才恍然大悟，这些应该都是男主的幻像。如果是这样的话，那么可以说的过去，而且还能表现出男主对家人的想念。在祖国发展的道路上，无数人都默默的奉献着自己，有的甚至都没人知道名字，他们不是不想家，而是心里有更大的家，他们都很伟大。想起高中时候背的一些感动中国素材里有句颁奖词：不是绝情，是极致的深情。不是冲动，是不悔的抉择。\n3. 夺冠 导演： 徐峥\n演员： 刘涛， 吴京， 徐峥， 还有两个可爱的小孩子\n这是关于女排第一次夺冠的事情。两个小朋友，女孩要去国外了，男孩不舍，想送个东西给女孩。看来每个男孩的心里都有着这样的一个女孩。这虽然不是爱情，却又不像是纯粹的友情，应该是那个年龄段男生女生之间的一种特殊的情感——就是想跟你一起玩耍，你走了，心里会空荡荡的。当然女孩很漂亮，刘涛也很漂亮。这段是想表现普通老百姓的国家荣誉感，大家一起凑在黑白电视机前看比赛，赢球时候的欢呼，夺冠之后的喜悦，更像是把国家的荣誉看成是自己的荣誉，好像是自己刚刚夺冠了一样。\n4. 回归 导演： 薛晓路\n演员： 杜江，朱一龙，惠英红， 高亚麟\n讲香港回归的故事。多次出现手表，也有升国旗仪式，和第一段有点相似。不同的是，这次强调的是香港回归，我们一秒都不愿多等。\n5 . 北京你好 导演： 宁浩\n主演： 葛优\n你大爷还是你大爷。演的很搞笑。讲的是08年奥运会的事。北京小市民（出租司机葛大爷）离婚了，抽到一张开幕式门票，到处显摆。人家出多少钱都不卖，也是把奥运看得很重要。最后却把票给了一个素不相识的四川小伙子。汶川地震也刚好在08年，小伙子的爸爸不幸死在了地震中，小伙子想看看爸爸参与建的鸟巢。最后的采访很巧妙，让其他人也知道了葛大爷的善举。\n6. 白昼流星 导演： 陈凯歌\n演员： 刘昊然，陈飞宇，江珊，田壮壮\n讲的是神州十一号飞船返回舱成功着陆的事。故事一开始，根本看不出跟飞船有啥关系。\n7. 护航 导演： 文牧野\n演员： 宋佳\n讲的是2015年9月3号的抗战胜利70周年的阅兵。女兵好帅啊\n总结 总的来说，这部电影拍得很不错，等于说是一个命题式的作文，作文的题目就是\u0026quot;我和我的祖国\u0026quot;，7个导演要求以此为题写一篇20Min的电影。”我“ 是指小人物，”我的祖国“是大背景，也就是说小老百姓和祖国的一些关联。发生的事情都是祖国建国到现在经历的一些大事，但是故事的主角却是这么一个大背景下的普通老百姓生活的点点滴滴。个人认为，最好的一个是夺冠，小姑娘太好看了。\n回宿舍的路上，我一直单曲循环播放王菲唱的”我和我的祖国“，声音真好听。马上就到2019年10月1日0点了，我祝我的祖国安稳昌盛，无忧无患。\n","permalink":"https://xiergo.github.io/posts/life/%E6%88%91%E5%92%8C%E6%88%91%E7%9A%84%E7%A5%96%E5%9B%BD/","summary":"","title":"我和我的祖国"},{"content":"问题描述 利用cibersort 工具进行deconvolution时候，由于LM22 signature矩阵是利用array数据得到的，在分析RNA-seq数据的时候性能有所下降，为此有文献就提出将RNA-Seq数据转化为array-like的数据进行deconvolution结果可能会好一点。那么怎么转化呢？\n解决方案 文献的附件里有提到方法的部分细节，如下图所示：\n即先将TCGA里的同时包含rna-seq和array的数据下载下来，一共550左右个样本。然后将TPM化为$log_2(TPM+1)$ ,再进行cubic smoothing spline拟合y~x之间的方程，这里的自由度文献给出了：df=4。\n至于什么是spline，自行google，文末有个用r进行spline的教程，里面简要的介绍了一部分，其实我也不是很清楚，时间有限就不深究了。\n值得注意的是，文献中有个关键词不能忽略了，gene-specific， 即对于每一个rna-seq和array的共同的基因，都有一条特异的拟合曲线。\n至于如何进行cubic smoothing spline，有现成的R包，splines，这是一个基础包，你不用再安装了，直接library了就能用，里面的spline.smooth()函数直接就能做这个，非常简单。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 library(\u0026#39;splines\u0026#39;) wdir=\u0026#39;/home/pub/project/deconvolution/tcga/splineScatterPlot\u0026#39; options(StringsAsFactors=F) setwd(wdir) if (\u0026#39;tmp1.rda\u0026#39; %in% list.files()) load(\u0026#39;tmp1.rda\u0026#39;) else { ov.array=read.delim(\u0026#39;/home/pub/project/deconvolution/results/Transcriptome/dataset/TCGA/microarray/OV/OV.array.geneExp.txt\u0026#39;) gbm.array=read.delim(\u0026#39;/home/pub/project/deconvolution/results/Transcriptome/dataset/TCGA/microarray/GBM/GBM.array.geneExp.txt\u0026#39;) lusc.array=read.delim(\u0026#39;/home/pub/project/deconvolution/results/Transcriptome/dataset/TCGA/microarray/LUSC/LUSC.array.geneExp.txt\u0026#39;) hk.gene=read.delim(\u0026#34;/home/pub/project/deconvolution/results/Transcriptome/dataset/TCGA/microarray/houseKeeping.gene.txt\u0026#34;,header = F) load(\u0026#39;/home/pub/project/deconvolution/tcga/exprTPM/exprT.rda\u0026#39;) save(list = ls(),file = \u0026#39;tmp1.rda\u0026#39;) } dim(ov.array) dim(gbm.array) dim(lusc.array) mi.array1=merge(ov.array,gbm.array,by=\u0026#39;Gene_symbol\u0026#39;,all=T) mi.array1=merge(mi.array1,lusc.array,by=\u0026#39;Gene_symbol\u0026#39;,all=T) dim(mi.array1) mi.array=mi.array1[,-1] rownames(mi.array)=mi.array1[,1] head(mi.array[,1:4]) sid=names(mi.array) sid=substring(sid,1,15) sid=gsub(\u0026#39;.\u0026#39;,\u0026#39;-\u0026#39;,sid,fixed = T) print(sid[1:4]) print(length(sid)) mi.array=mi.array[,!duplicated(sid)] sid=unique(sid) print(length(sid)) ind=match(sid,names(exprT)) mi.array=mi.array[,which(!is.na(ind))] ind=ind[!is.na(ind)] tpm=exprT[,ind] rownames(tpm)=exprT[,1] dim(tpm) dim(mi.array) tpm=as.matrix(tpm) mi.array=as.matrix(mi.array) tpm[1:3,1:3] mi.array[1:3,1:3] ## 原始的为log2(tpm+0.5)，处理成log2(tpm+1) print(\u0026#39;start plot~~~~~~~\u0026#39;) tpm=log2(2^tpm+0.5) gene=as.character(hk.gene$V1) print(gene) for(g in gene){ print(g) if(g %in% rownames(tpm) \u0026amp; g %in% rownames(mi.array)){ x=tpm[rownames(tpm)==g,] y=mi.array[rownames(mi.array)==g,] not.na=(!is.na(x))\u0026amp;(!is.na(y)) n=length(not.na) print(n) x=x[not.na] y=y[not.na] fit=smooth.spline(x,y,df=4) main=paste0(g,\u0026#39; (\u0026#39;,n,\u0026#39; samples)\u0026#39;) png(paste0(g,\u0026#39;.png\u0026#39;),height=500,width=500) plot(x,y,main=main,xlab=\u0026#39;log2(TPM+1)\u0026#39;,ylab=\u0026#39;microarray\u0026#39;) lines(fit,lwd=2,col=\u0026#34;purple\u0026#34;) legend(\u0026#34;topright\u0026#34;,(\u0026#34;Smoothing Splines with 4 df\u0026#34;),col=\u0026#34;purple\u0026#34;,lwd=2) dev.off() }else{ cat(\u0026#39;\\n\u0026#39;,g,\u0026#39;is not included in tpm and mi.array\\n\u0026#39;) } } 代码的前半部分为数据的清洗和处理，每个人的数据来源和数据格式都不一样，所有这部分会略有差异。\n结果展示 我挑了其中的2个管家基因的散点图以及拟合曲线进行展示。\nreference Cubic and Smoothing Splines in R\nPan-cancer Immunogenomix Analyses Reveal Genotype-Immunophenotype Relationships of Response to Checkpoint Blackade\n","permalink":"https://xiergo.github.io/posts/tech/cubic_smoothing_spline/","summary":"","title":"Cubic_smoothing_spline"},{"content":"问题描述 pheatmap包是R绘图包里的重要一员，在绘制热图方面有着独特的优势，学生信的人基本都用过。可是一般很少用heatmap画离散变量，尤其是画完离散变量后，加个legend非常麻烦。今天我遇到了这个需求：\n下图是用pheatmap函数画的图，用的matrix是 Pearson correlation，取值[-1, 1]， 灰色代表NA值，红色越深，约接近1；蓝色越深，越接近0 。\n老师又让我画一个离散化的热图。就是用一系列的值将[-1，1]区间截断，每个小区间对应一个颜色。如：颜色设置归档：[0.8,1]，[0.6,0.8)，[0.4,0.6)，[0.2,0.4)，[-1,0.2)。\n结果展示 我在原函数的基础上写了一个新函数pheatmap.discre()，做了如下的图：\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 library(pheatmap) # x : matrix # sep.vec : seperator vector, such as c(0.2,0.4,0.6,0.8) # sep.type: either \u0026#39;[\u0026#39; or \u0026#39;]\u0026#39;, if \u0026#39;[\u0026#39;, [-1,0.2),[0.2,0.4)......[0.8,1] # col: color # min: minimum of x # max: maximum of x # ... : extra arguments passed to pheatmap. pheatmap.discre=function(x,sep.vec,sep.type,col,min,max,...){ discre=function(x,v,type=\u0026#39;]\u0026#39;){ if(any(duplicated(v))) stop(\u0026#39;items in v should be unique\u0026#39;) if(!type %in% c(\u0026#39;[\u0026#39;,\u0026#39;]\u0026#39;)) stop(\u0026#39;type can only be \u0026#34;[\u0026#34; or \u0026#34;]\u0026#34;\u0026#39;) if(is.na(x)) return(NA) a=c(x,v) a=which(x==sort(a))-1 if(type==\u0026#39;]\u0026#39;) a=min(a) else a=max(a) return(a) } disc.color=function(col,max,min,sep.vec){ n=length(col) v=c(min,sort(sep.vec),max) v1=(v[-1]+v[-length(v)])/2 v1=(v1-min)/(max-min) v1=round(1+(n-1)*v1) return(col[v1]) } args=list(...) sep.vec=sort(sep.vec) x=apply(x,1:2, discre,v=sep.vec,type=sep.type) col=disc.color(col,max,min,sep.vec) x.max=max(x,na.rm = T) x.min=min(x,na.rm = T) col=col[(x.min:x.max)+1] print(col) n=length(col) d.legend=(x.max-x.min)/n legend_breaks=seq(from=x.min+0.5*d.legend,by=d.legend,length.out = n) legend_labels=paste(c(min,sep.vec),\u0026#39;~\u0026#39;,c(sep.vec,max)) legend_labels=legend_labels[(x.min:x.max)+1] print(legend_breaks) print(legend_labels) a=list(x,col,legend_labels,legend_breaks) names(a)=c(\u0026#39;mat\u0026#39;,\u0026#39;color\u0026#39;,\u0026#39;legend_labels\u0026#39;,\u0026#39;legend_breaks\u0026#39;) args=c(a,args) do.call(pheatmap,args) } ","permalink":"https://xiergo.github.io/posts/tech/discrete-pheatmap/","summary":"","title":"Discrete Pheatmap"},{"content":"Train_url\n","permalink":"https://xiergo.github.io/posts/tech/train_url/","summary":"Train_url","title":""}]